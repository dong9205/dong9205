[{"categories":["documentation","kubernetes"],"content":"kubeadm高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:0:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"基础环境初始化 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装Docker,Containerd 配置先决条件 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 通过运行以下指令确认 br_netfilter 和 overlay 模块被加载： lsmod | grep br_netfilter lsmod | grep overlay 通过运行以下指令确认 net.bridge.bridge-nf-call-iptables、net.bridge.bridge-nf-call-ip6tables 和 net.ipv4.ip_forward 系统变量在你的 sysctl 配置中被设置为 1： sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 安装docker，containerd sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin rm -f /etc/containerd/config.toml systemctl start docker 修改containerd配置 报错: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service 需要将SystemdCgroup修改为true: https://github.com/kubernetes/kubeadm/issues/1047 containerd config default \u003e /etc/containerd/config.toml sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装kubeadm 关闭swap 如果 kubelet 未被正确配置使用交换分区，则你必须禁用交换分区。 例如，sudo swapoff -a 将暂时禁用交换分区。要使此更改在重启后保持不变，请确保在如 /etc/fstab、systemd.swap 等配置文件中禁用交换分区，具体取决于你的系统如何配置。 安装 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config sudo yum install -y kubelet-1.27.6 kubeadm-1.27.6 kubectl-1.27.6 --disableexcludes=kubernetes sudo systemctl enable --now kubelet ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"使用内部etcd构建高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"kube-apiserver负载均衡器 使用haproxy+keepalived的方式做apiserver高可用 机器 IP 安装步骤 master1 192.168.25.3 kubeadm安装 master2 192.168.25.5 kubeadm安装 node1 192.168.25.4 kubeadm安装 VIP 192.168.25.6 kubeadm安装 配置haproxy 配置kubelet的静态pod yaml文件 cat /etc/kubernetes/manifests/haproxy.yaml apiVersion: v1 kind: Pod metadata: name: haproxy namespace: kube-system spec: containers: #- image: haproxy:2.1.4 - image: haproxy:2.8.4 name: haproxy livenessProbe: failureThreshold: 8 httpGet: host: localhost path: /healthz port: 30033 scheme: HTTPS volumeMounts: - mountPath: /usr/local/etc/haproxy/haproxy.cfg name: haproxyconf readOnly: true hostNetwork: true volumes: - hostPath: path: /etc/haproxy/haproxy.cfg type: FileOrCreate name: haproxyconf 配置haproxy配置 cat /etc/haproxy/haproxy.cfg # /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log stdout local0 log stdout local1 notice daemon maxconn 75000 #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 1 timeout http-request 10s timeout queue 20s timeout connect 5s timeout client 20s timeout server 20s timeout http-keep-alive 10s timeout check 10s #--------------------------------------------------------------------- # apiserver frontend which proxys to the control plane nodes #--------------------------------------------------------------------- frontend apiserver bind *:30033 mode tcp option tcplog default_backend apiserver #--------------------------------------------------------------------- # round robin balancing for apiserver #--------------------------------------------------------------------- backend apiserver option httpchk GET /healthz http-check expect status 200 mode tcp option ssl-hello-chk balance roundrobin server master1 192.168.25.3:6443 check server master2 192.168.25.5:6443 check 配置keepalived 配置kubelet管理静态POD文件 cat /etc/kubernetes/manifests/keepalived.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: keepalived namespace: kube-system spec: containers: - image: osixia/keepalived:stable name: keepalived resources: {} args: # Fix docker mounted file problems: https://github.com/osixia/docker-keepalived#fix-docker-mounted-file-problems - --copy-service securityContext: capabilities: add: - NET_ADMIN - NET_BROADCAST - NET_RAW volumeMounts: - mountPath: /container/service/keepalived/assets/keepalived.conf name: config - mountPath: /container/service/keepalived/assets/check_apiserver.sh name: check hostNetwork: true volumes: - hostPath: path: /etc/keepalived/keepalived.conf name: config - hostPath: path: /etc/keepalived/check_apiserver.sh name: check 配置keepalived cat /etc/keepalived/keepalived.conf ! /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_apiserver { script \"/container/service/keepalived/assets/check_apiserver.sh\" interval 3 weight -20 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 authentication { auth_type PASS auth_pass private } virtual_ipaddress { 192.168.25.6/26 dev eth0 label eth0:1 } unicast_src_ip 192.168.25.3 unicast_peer { 192.168.25.5 } track_script { check_apiserver } } 配置检测脚本 cat /etc/keepalived/check_apiserver.sh #!/bin/sh errorExit() { echo \"*** $*\" 1\u003e\u00262 exit 1 } curl --silent --max-time 2 --insecure https://localhost:30033/ -o /dev/null || errorExit \"Error GET https://localhost:30033/\" if ip addr | grep -q 192.168.25.6; then curl --silent --max-time 2 --insecure https://192.168.25.6:30033/ -o /dev/null || errorExit \"Error GET https://192.168.25.6:30033/","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"其他问题 kubectl使用watch操作20秒就会断开 原因是因为haproxy设置的20超时，但是kubectl默认的 idle timeout是30秒，将kubectl的idel timeout配置为15秒即可. issue: https://github.com/kubernetes/kubeadm/issues/2227 export HTTP2_READ_IDLE_TIMEOUT_SECONDS=15 v1.27移除–pod-eviction-timeout后怎么修改驱逐POD时长 https://kubernetes.io/zh-cn/blog/2023/03/17/upcoming-changes-in-kubernetes-v1-27/#%E7%A7%BB%E9%99%A4-pod-eviction-timeout-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0 使用污点方式: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions tolerations: - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 120 下线节点 kubectl cordon node1 kubectl drain --ignore-daemonsets node1 # --ignore-daemonsets 选项为忽略DaemonSet类型的pod ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"参考文档 docker安装官方文档: https://docs.docker.com/engine/install/centos/ 容器运行时: https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ 安装 kubeadm: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 高可用拓扑选项: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/ 利用 kubeadm 创建高可用集群: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/ 阿里云使用keepalived: https://developer.aliyun.com/article/438705 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:3:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net ","date":"2024-11-11","objectID":"/versionmanager/:0:0","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"Go 多版本管理工具 文章目录 Go 多版本管理工具 一、go get 命令 1.1 使用方法： 二、Goenv 三、GVM (Go Version Manager) 四、voidint/g 4.1 安装 4.2 冲突 4.3 使用 在平时开发中，本地新旧项目并行开发的过程中，你大概率会遇到一个令人头疼的问题，如何同时使用两个不同版本的 Golang Runtime 进行开发呢？ ","date":"2024-11-11","objectID":"/versionmanager/:1:0","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"一、go get 命令 这种方法有一个前提，那就是当前系统中已经通过标准方法安装过某个版本的 Go 了。 1.1 使用方法： 在项目中初始化 Go Modules： go mod init \u003cmodule-name\u003e go 版本安装 / 版本切换, 安装不同版本的 Go： go get golang.org/dl/go\u003cx.y\u003e go\u003cx.y\u003e download go\u003cx.y\u003e version ![][img-0] 切换全局 Go 版本： go\u003cx.y\u003e use ","date":"2024-11-11","objectID":"/versionmanager/:1:1","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"二、Goenv 官网：https://github.com/go-nv/goenv Goenv 是另一个 Go 多版本管理工具，它的工作原理与其他语言的版本管理工具（如 Ruby 的 RVM 和 Python 的 pyenv）类似。以下是使用 Goenv 的基本步骤： 安装 Goenv（你需要先安装 Git）： git clone https://github.com/syndbg/goenv.git ~/.goenv 将 Goenv 添加到你的 shell 配置文件（例如 ~/.bashrc 或 ~/.zshrc）中： echo 'export GOENV_ROOT=\"$HOME/.goenv\"' \u003e\u003e ~/.bashrc echo 'export PATH=\"$GOENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(goenv init -)\"' \u003e\u003e ~/.bashrc 安装你需要的 Go 版本： goenv install go1.x.x 使用特定版本的 Go： goenv global go1.x.x ","date":"2024-11-11","objectID":"/versionmanager/:1:2","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"三、GVM (Go Version Manager) 官网：https://github.com/moovweb/gvm GVM 是一个流行的 Go 多版本管理工具，它允许你在同一台机器上安装和切换不同版本的 Go。以下是使用 GVM 的基本步骤： 安装 GVM： bash \u003c \u003c(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) 安装你需要的 Go 版本： gvm install go1.x.x 使用特定版本的 Go： gvm use go1.x.x 指定安装目录 apt-get install bison acl bsdmainutils bash -s master /opt \u003c \u003c(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) gvm install go1.23.2 -B gvm use go1.23.2 --default sudo setfacl -Rm d:u:${USER}:rwx /opt/go/ sudo setfacl -Rm u:${USER}:rwx /opt/go/ echo '[[ -s \"/opt//gvm/scripts/gvm\" ]] \u0026\u0026 source \"/opt/gvm/scripts/gvm\"' \u003e\u003e /etc/profile echo 'export GOPATH=/opt/go' \u003e\u003e /etc/profile ","date":"2024-11-11","objectID":"/versionmanager/:1:3","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"四、voidint/g 4.1 安装 g是一个 Linux、macOS、Windows 下的命令行工具，可以提供一个便捷的多版本 go 环境的管理和切换。 以下是使用 g 的基本步骤： Linux/macOS（适用于 bash、zsh） # 建议安装前清空`GOROOT`、`GOBIN`等环境变量 curl -sSL https://raw.githubusercontent.com/voidint/g/master/install.sh | bash echo \"unalias g\" \u003e\u003e ~/.bashrc # 可选。若其他程序（如'git'）使用了'g'作为别名。 source \"$HOME/.g/env\" Windows（适用于 pwsh） iwr https://raw.githubusercontent.com/voidint/g/master/install.ps1 -useb | iex 4.2 冲突 这里如果你是 oh-my-zsh 的用户，那么你还需要做一件事，就是解决全局的 g 命令的冲突，解决的方式有两种，第一种是在你的 .zshrc 文件末尾添加 unalias ： echo \"unalias g\" \u003e\u003e ~/.zshrc # 可选。若其他程序（如'git'）使用了'g'作为别名。 # 记得重启 shell ，或者重新 source 配置 第二种，则是调整 ~/.oh-my-zsh/plugins/git/git.plugin.zsh 中关于 g 的注册，将其注释或删除掉： # alias g='git' 我的 .zshrc 中的完整配置： # 我的 g 的bin目录调整到了 .gvm ,所以你可能需要一些额外的调整 export PATH=\"${HOME}/.gvm/bin:$PATH\" export GOROOT=\"${HOME}/.g/go\" export PATH=\"${HOME}/.g/go/bin:$PATH\" export G_MIRROR=https://gomirrors.org/ 4.3 使用 查询当前可供安装的stable状态的 go 版本 $ g ls-remote stable 1.19.10 1.20.5 安装目标 go 版本1.20.5 $ g install 1.14.7 Downloading 100% [===============] (92/92 MB, 12 MB/s) Computing checksum with SHA256 Checksums matched Now using go1.20.5 查询已安装的 go 版本 $ g ls 1.19.10 * 1.20.5 查询可供安装的所有 go 版本 $ g ls-remote 1 1.2.2 1.3 1.3.1 ... // 省略若干版本 1.19.10 1.20rc1 1.20rc2 1.20rc3 1.20 1.20.1 1.20.2 1.20.3 1.20.4 * 1.20.5 切换到另一个已安装的 go 版本 $ g use 1.19.10 go version go1.19.10 darwin/arm64 卸载一个已安装的 go 版本 $ g uninstall 1.19.10 Uninstalled go1.19.10 清空 go 安装包文件缓存 $ g clean Remove go1.18.10.darwin-arm64.tar.gz Remove go1.19.10.darwin-arm64.tar.gz Remove go1.20.5.darwin-arm64.tar.gz 查看 g 版本信息 g version 1.5.0 build: 2023-01-01T21:01:52+08:00 branch: master commit: cec84a3f4f927adb05018731a6f60063fd2fa216 更新 g 软件本身 $ g self update You are up to date! g v1.5.0 is the latest version. 卸载 g 软件本身 $ g self uninstall Are you sure you want to uninstall g? (Y/n) y Remove /Users/voidint/.g/bin/g Remove /Users/voidint/.g 总之，选择其中一个工具并根据你的需求进行设置。这些工具都可以有效地管理不同版本的 Go Runtime，使你能够轻松地在不同项目中切换和使用不同的 Go 版本。 ","date":"2024-11-11","objectID":"/versionmanager/:1:4","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"参考文档 https://blog.csdn.net/weixin_44621343/article/details/133148154 ","date":"2024-11-11","objectID":"/versionmanager/:2:0","tags":[""],"title":"Go 多版本管理工具","uri":"/versionmanager/"},{"categories":["documentation"],"content":"介绍 ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:0:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"Nginx ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:1:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"安装 官方文档: https://nginx.org/en/linux_packages.html#Ubuntu sudo apt install curl gnupg2 ca-certificates lsb-release ubuntu-keyring curl https://nginx.org/keys/nginx_signing.key | gpg --dearmor \\ | sudo tee /usr/share/keyrings/nginx-archive-keyring.gpg \u003e/dev/null gpg --dry-run --quiet --no-keyring --import --import-options import-show /usr/share/keyrings/nginx-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/nginx-archive-keyring.gpg] \\ http://nginx.org/packages/ubuntu `lsb_release -cs` nginx\" \\ | sudo tee /etc/apt/sources.list.d/nginx.list echo -e \"Package: *\\nPin: origin nginx.org\\nPin: release o=nginx\\nPin-Priority: 900\\n\" \\ | sudo tee /etc/apt/preferences.d/99nginx sudo apt update sudo apt install nginx -y systemctl start nginx ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:1:1","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"配置Nginx 编辑nginx.conf，增加以下配置 vim /etc/nginx/nginx.conf map \"$time_iso8601 # $msec\" $time_iso8601_ms { \"~(^[^+]+)(\\+[0-9:]+) # \\d+\\.(\\d+)$\" $1.$3$2; } log_format main '{\"timestamp\":\"$time_iso8601_ms\",' '\"server_ip\":\"$server_addr\",' '\"remote_ip\":\"$remote_addr\",' '\"xff\":\"$http_x_forwarded_for\",' '\"remote_user\":\"$remote_user\",' '\"domain\":\"$host\",' '\"url\":\"$request_uri\",' '\"referer\":\"$http_referer\",' '\"upstreamtime\":\"$upstream_response_time\",' '\"responsetime\":\"$request_time\",' '\"request_method\":\"$request_method\",' '\"status\":\"$status\",' '\"response_length\":\"$bytes_sent\",' '\"request_length\":\"$request_length\",' '\"protocol\":\"$server_protocol\",' '\"upstreamhost\":\"$upstream_addr\",' '\"http_user_agent\":\"$http_user_agent\"' '}'; ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:1:2","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"重载配置 systemctl start nginx ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:1:3","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"安装Docker https://docs.docker.com/engine/install/ubuntu/ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:2:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"Clickhouse ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"创建部署目录和docker-compose.yaml mkdir -p /opt/clickhouse/etc/clickhouse-server/{config.d,users.d} cd /opt/clickhouse cat \u003c\u003c-EOF \u003e docker-compose.yaml services: clickhouse: image: registry.cn-shenzhen.aliyuncs.com/starsl/clickhouse-server:23.4 container_name: clickhouse hostname: clickhouse volumes: - /opt/clickhouse/logs:/var/log/clickhouse-server - /opt/clickhouse/data:/var/lib/clickhouse - /opt/clickhouse/etc/clickhouse-server/config.d/config.xml:/etc/clickhouse-server/config.d/config.xml - /opt/clickhouse/etc/clickhouse-server/users.d/users.xml:/etc/clickhouse-server/users.d/users.xml - /usr/share/zoneinfo/PRC:/etc/localtime ports: - 8123:8123 - 9000:9000 EOF ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:1","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"配置主文件 vim /opt/clickhouse/etc/clickhouse-server/config.d/config.xml \u003cclickhouse replace=\"true\"\u003e \u003clogger\u003e \u003clevel\u003edebug\u003c/level\u003e \u003clog\u003e/var/log/clickhouse-server/clickhouse-server.log\u003c/log\u003e \u003cerrorlog\u003e/var/log/clickhouse-server/clickhouse-server.err.log\u003c/errorlog\u003e \u003csize\u003e1000M\u003c/size\u003e \u003ccount\u003e3\u003c/count\u003e \u003c/logger\u003e \u003cdisplay_name\u003ech_accesslog\u003c/display_name\u003e \u003clisten_host\u003e0.0.0.0\u003c/listen_host\u003e \u003chttp_port\u003e8123\u003c/http_port\u003e \u003ctcp_port\u003e9000\u003c/tcp_port\u003e \u003cuser_directories\u003e \u003cusers_xml\u003e \u003cpath\u003eusers.xml\u003c/path\u003e \u003c/users_xml\u003e \u003clocal_directory\u003e \u003cpath\u003e/var/lib/clickhouse/access/\u003c/path\u003e \u003c/local_directory\u003e \u003c/user_directories\u003e \u003c/clickhouse\u003e ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:2","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"配置用户文件 PASSWORD=$(base64 \u003c /dev/urandom | head -c8); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha256sum | tr -d '-' 42ieYiyE 75f5da2928c7fd93ac68a4db853bb4b6374b1f169fbb49aff3c068b061921d84 vim /opt/clickhouse/etc/clickhouse-server/users.d/users.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse replace=\"true\"\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_memory_usage\u003e10000000000\u003c/max_memory_usage\u003e \u003cuse_uncompressed_cache\u003e0\u003c/use_uncompressed_cache\u003e \u003cload_balancing\u003ein_order\u003c/load_balancing\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003c/default\u003e \u003c/profiles\u003e \u003cusers\u003e \u003cdefault\u003e \u003cpassword remove='1' /\u003e \u003cpassword_sha256_hex\u003e填写生成的密码密文{75f5da2928c7fd93ac68a4db853bb4b6374b1f169fbb49aff3c068b061921d84}\u003c/password_sha256_hex\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003cprofile\u003edefault\u003c/profile\u003e \u003cnetworks\u003e \u003cip\u003e::/0\u003c/ip\u003e \u003c/networks\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003cnamed_collection_control\u003e1\u003c/named_collection_control\u003e \u003cshow_named_collections\u003e1\u003c/show_named_collections\u003e \u003cshow_named_collections_secrets\u003e1\u003c/show_named_collections_secrets\u003e \u003c/default\u003e \u003c/users\u003e \u003cquotas\u003e \u003cdefault\u003e \u003cinterval\u003e \u003cduration\u003e3600\u003c/duration\u003e \u003cqueries\u003e0\u003c/queries\u003e \u003cerrors\u003e0\u003c/errors\u003e \u003cresult_rows\u003e0\u003c/result_rows\u003e \u003cread_rows\u003e0\u003c/read_rows\u003e \u003cexecution_time\u003e0\u003c/execution_time\u003e \u003c/interval\u003e \u003c/default\u003e \u003c/quotas\u003e \u003c/clickhouse\u003e ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:3","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"启动 docker compose up -d ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:4","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"创建数据库与表 进入ck数据库 docker container exec -it clickhouse clickhouse-client --user default --password 42ieYiyE 执行创建语句 CREATE DATABASE IF NOT EXISTS nginxlogs ENGINE=Atomic; CREATE TABLE nginxlogs.nginx_access ( `timestamp` DateTime64(3, 'Asia/Shanghai'), `server_ip` String, `domain` String, `request_method` String, `status` Int32, `top_path` String, `path` String, `query` String, `protocol` String, `referer` String, `upstreamhost` String, `responsetime` Float32, `upstreamtime` Float32, `duration` Float32, `request_length` Int32, `response_length` Int32, `client_ip` String, `client_latitude` Float32, `client_longitude` Float32, `remote_user` String, `remote_ip` String, `xff` String, `client_city` String, `client_region` String, `client_country` String, `http_user_agent` String, `client_browser_family` String, `client_browser_major` String, `client_os_family` String, `client_os_major` String, `client_device_brand` String, `client_device_model` String, `createdtime` DateTime64(3, 'Asia/Shanghai') ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(timestamp) PRIMARY KEY (timestamp, server_ip, status, top_path, domain, upstreamhost, client_ip, remote_user, request_method, protocol, responsetime, upstreamtime, duration, request_length, response_length, path, referer, client_city, client_region, client_country, client_browser_family, client_browser_major, client_os_family, client_os_major, client_device_brand, client_device_model ) TTL toDateTime(timestamp) + toIntervalDay(30) SETTINGS index_granularity = 8192; ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:3:5","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"部署Vector采集日志 ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:4:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"Vector部署 # 创建部署目录和docker-compose.yaml mkdir -p /opt/vector/conf cd /opt/vector touch access_vector_error.log wget https://raw.githubusercontent.com/P3TERX/GeoLite.mmdb/download/GeoLite2-City.mmdb cat \u003c\u003c-EOF \u003e docker-compose.yaml services: vector: image: registry.cn-shenzhen.aliyuncs.com/starsl/vector:0.41.1-alpine container_name: vector hostname: vector restart: always entrypoint: vector --config-dir /etc/vector/conf ports: - 8686:8686 volumes: - /var/log/nginx:/nginx_logs # 这是需要采集的日志的路径需要挂载到容器内 - /opt/vector/access_vector_error.log:/tmp/access_vector_error.log - /opt/vector/GeoLite2-City.mmdb:/etc/vector/GeoLite2-City.mmdb - /opt/vector/conf:/etc/vector/conf - /usr/share/zoneinfo/PRC:/etc/localtime EOF ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:4:1","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"Vector配置 cd /opt/vector/conf cat \u003c\u003c-EOF \u003e vector.yaml timezone: \"Asia/Shanghai\" api: enabled: true address: \"0.0.0.0:8686\" EOF vim nginx-access.yaml sources: 01_file_nginx_access: type: file include: - /nginx_logs/access.log #nginx请求日志路径 transforms: 02_parse_nginx_access: drop_on_error: true reroute_dropped: true type: remap inputs: - 01_file_nginx_access source: | .message = string!(.message) if contains(.message,\"\\\\x\") { .message = replace(.message, \"\\\\x\", \"\\\\\\\\x\") } . = parse_json!(.message) .createdtime = to_unix_timestamp(now(), unit: \"milliseconds\") .timestamp = to_unix_timestamp(parse_timestamp!(.timestamp , format: \"%+\"), unit: \"milliseconds\") .url_list = split!(.url, \"?\", 2) .path = .url_list[0] .query = .url_list[1] .path_list = split!(.path, \"/\", 3) if length(.path_list) \u003e 2 {.top_path = join!([\"/\", .path_list[1]])} else {.top_path = \"/\"} .duration = round(((to_float(.responsetime) ?? 0) - (to_float(.upstreamtime) ?? 0)) ?? 0,3) if .xff == \"-\" { .xff = .remote_ip } .client_ip = split!(.xff, \",\", 2)[0] .ua = parse_user_agent!(.http_user_agent , mode: \"enriched\") .client_browser_family = .ua.browser.family .client_browser_major = .ua.browser.major .client_os_family = .ua.os.family .client_os_major = .ua.os.major .client_device_brand = .ua.device.brand .client_device_model = .ua.device.model .geoip = get_enrichment_table_record(\"geoip_table\", {\"ip\": .client_ip}) ?? {\"city_name\":\"unknown\",\"region_name\":\"unknown\",\"country_name\":\"unknown\"} .client_city = .geoip.city_name .client_region = .geoip.region_name .client_country = .geoip.country_name .client_latitude = .geoip.latitude .client_longitude = .geoip.longitude del(.path_list) del(.url_list) del(.ua) del(.geoip) del(.url) sinks: 03_ck_nginx_access: type: clickhouse inputs: - 02_parse_nginx_access endpoint: http://10.7.0.26:8123 #clickhouse http接口 database: nginxlogs #clickhouse 库 table: nginx_access #clickhouse 表 auth: strategy: basic user: default #clickhouse 库 password: GlWszBQp #clickhouse 密码 compression: gzip 04_out_nginx_dropped: type: file inputs: - 02_parse_nginx_access.dropped path: /tmp/access_vector_error.log #解析异常的日志 encoding: codec: json enrichment_tables: geoip_table: path: \"/etc/vector/GeoLite2-City.mmdb\" type: geoip locale: \"zh-CN\" ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:4:2","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"运行Vector docker compose up -d ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:4:3","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"grafana ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:5:0","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"运行grafana docker run -d --name=grafana -p 3000:3000 grafana/grafana ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:5:1","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"配置ClickHouse 安装插件 docker container exec -it grafana grafana cli plugins install grafana-clickhouse-datasource docker restart grafana 增加数据源 导入看板 Grafana 请求日志分析看板预览 该看板是基于 ClickHouse + Vector 的 NGINX 请求日志分析看板。包括请求与耗时分析、异常请求分析、用户分析、地理位置分布图、指定接口分析、请求日志明细。 尤其在异常请求分析方面，总结多年异常请求分析经验，从各个角度设计大量异常请求的分析图表。 整体请求与耗时分析 NGINX 异常请求分析 ![](./NGINX 异常请求分析.png) 用户请求数据分析 地理位置数据分析 指定接口明细分析 请求日志详情分析 参考文档 https://mp.weixin.qq.com/s/6VSwFCfK0G_QQUjMnLs9Dw ","date":"2024-11-06","objectID":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/:5:2","tags":[""],"title":"CH+Vector打造最强Grafana日志分析","uri":"/ch_vector%E6%89%93%E9%80%A0%E6%9C%80%E5%BC%BAgrafana%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"categories":["documentation"],"content":"SAML登录异常错误记录 ","date":"2024-10-11","objectID":"/ssoerror/:0:0","tags":[""],"title":"单点登录报错记录","uri":"/ssoerror/"},{"categories":["documentation"],"content":"Missing signature referencing the top-level element ","date":"2024-10-11","objectID":"/ssoerror/:1:0","tags":[""],"title":"单点登录报错记录","uri":"/ssoerror/"},{"categories":["documentation"],"content":"原因 使用的Google的Saml协议接入单点登录, 后端配置了验证，需要勾选\"已签署响应复选框\" ","date":"2024-10-11","objectID":"/ssoerror/:1:1","tags":[""],"title":"单点登录报错记录","uri":"/ssoerror/"},{"categories":["documentation"],"content":"排查细节 首先通过报错发现了具体的错误函数，发现是通过调用了goxmldsig.NewDefaultValidationContext创建的ValidationContext结构体ctx,然后ctx的Validate函数的报错 继续Debug，发现是findSignature下sig的结果为空，导致返回报错\"Missing signature referencing the top-level element\" sig为空的原因是\"idAttr\"和\"ref.URI\"的不相同. 然后找到其他正常的请求发现XML里边有ds:Signature字段，但是Google的saml返回结果里边没有这个字段 然后在平台上找到\"已签署响应复选框“选项 ","date":"2024-10-11","objectID":"/ssoerror/:1:2","tags":[""],"title":"单点登录报错记录","uri":"/ssoerror/"},{"categories":["documentation"],"content":"gost配置反向代理隧道 ","date":"2024-09-11","objectID":"/gost/:0:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"初始化 mkdir -p /opt/go/src/github.com/go-gost cd /opt/go/src/github.com/go-gost git@github.com:go-gost/gost.git mkdir gost-plus cd gost-plus/ ","date":"2024-09-11","objectID":"/gost/:1:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"配置gost.yaml services: - name: service-0 addr: :8080 handler: type: tunnel metadata: entrypoint: :8000 ingress: ingress-0 #scheme: https listener: type: ws ingresses: - name: ingress-0 plugin: type: grpc addr: gost-plugins:8000 log: level: info ","date":"2024-09-11","objectID":"/gost/:2:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"配置docker-compose.yaml version: '3' services: gost-tunnel: image: gogost/gost restart: always ports: - \"8081:8081\" - \"8082:8082\" volumes: - /opt/go/src/github.com/go-gost/gost-plus/gost-plus:/etc/gost/ gost-plugins: image: ginuerzh/gost-plugins restart: always command: \"ingress --addr=:8082 --redis.addr=redis:6379 --redis.db=2 --redis.expiration=24h --domain=gost.p-pp.cn --log.level=debug\" redis: image: redis:7.2.1-alpine restart: always command: \"redis-server --save 60 1 --loglevel warning\" volumes: - redis:/data volumes: redis: driver: local ","date":"2024-09-11","objectID":"/gost/:3:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"启动 docker compose up -d ","date":"2024-09-11","objectID":"/gost/:4:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"配置nginx cat /etc/nginx/sites-enabled/tunnel.gost.p-pp.cn.conf upstream internal.infra.gost { server localhost:8081; } server { listen 80; server_name tunnel.gost.p-pp.cn; return 308 https://$host$request_uri; } server { listen 443 ssl; server_name tunnel.gost.p-pp.cn; ssl_certificate gost-cert/tunnel.gost.p-pp.cn.pem; ssl_certificate_key gost-cert/tunnel.gost.p-pp.cn.key; ssl_protocols TLSv1.2; ssl_ciphers ECDHE-RSA-AES256-SHA384:AES256-SHA256:RC4:HIGH:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!AESGCM; ssl_session_cache shared:SSL_WS2:500m; ssl_session_timeout 10m; ssl_prefer_server_ciphers on; access_log /var/log/nginx/tunnel.gost.p-pp.cn.access.log body_main; error_log /var/log/nginx/tunnel.gost.p-pp.cn.access.log; location / { proxy_pass_header server; proxy_set_header host $http_host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header x-real-ip $remote_addr; proxy_set_header x-scheme $scheme; proxy_http_version 1.1; proxy_set_header Connection keep-alive; proxy_set_header Keep-Alive 3600; keepalive_timeout 3600; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_pass http://internal.infra.gost; } } cat /etc/nginx/sites-enabled/gost.p-pp.cn.conf upstream internal.infra.gost-plugin { server localhost:8082; } server { listen 80; server_name *.gost.p-pp.cn; return 308 https://$host$request_uri; } server { listen 443 ssl; server_name *.gost.p-pp.cn; ssl_certificate gost-cert/tunnel.gost.p-pp.cn.pem; ssl_certificate_key gost-cert/tunnel.gost.p-pp.cn.key; #ssl_certificate gost-cert/$ssl_server_name.pem; #ssl_certificate_key gost-cert/$ssl_server_name.key; ssl_protocols TLSv1.2; ssl_ciphers ECDHE-RSA-AES256-SHA384:AES256-SHA256:RC4:HIGH:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!AESGCM; ssl_session_cache shared:SSL_WS2:500m; ssl_session_timeout 10m; ssl_prefer_server_ciphers on; access_log /var/log/nginx/gost.p-pp.cn.access.log main; error_log /var/log/nginx/gost.p-pp.cn.access.log; location / { proxy_pass_header server; proxy_set_header host $http_host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header x-real-ip $remote_addr; proxy_set_header x-scheme $scheme; proxy_http_version 1.1; proxy_set_header Connection keep-alive; proxy_set_header Keep-Alive 3600; keepalive_timeout 3600; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_pass http://internal.infra.gost-plugin; } } ","date":"2024-09-11","objectID":"/gost/:5:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"配置域名解析 域名 类型 地址 gost.p-pp.cn CNAME host02.p-pp.cn *.gost.p-pp.cn CNAME host02.p-pp.cn ","date":"2024-09-11","objectID":"/gost/:6:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"客户端下载 https://github.com/go-gost/gost/releases ","date":"2024-09-11","objectID":"/gost/:7:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"客户端连接 ./gost -L rtcp://:/127.0.0.1:3000 -F \"tunnel+wss://tunnel.gost.p-pp.cn:443\" {\"handler\":\"rtcp\",\"kind\":\"service\",\"level\":\"info\",\"listener\":\"rtcp\",\"msg\":\"listening on :0/tcp\",\"service\":\"service-0\",\"time\":\"2024-09-11T21:30:01.587+08:00\"} {\"connector\":\"tunnel\",\"dialer\":\"wss\",\"endpoint\":\"065d67d07b0a820f\",\"hop\":\"hop-0\",\"kind\":\"connector\",\"level\":\"info\",\"msg\":\"create tunnel on 065d67d07b0a820f:0/tcp OK, tunnel=d101599e-7040-11ef-a7f4-c4c6e6b12218, connector=03a48873-fdbd-4b49-a034-f517d2e69c29, weight=0\",\"node\":\"node-0\",\"time\":\"2024-09-11T21:30:01.656+08:00\",\"tunnel\":\"d101599e-7040-11ef-a7f4-c4c6e6b12218\"} ","date":"2024-09-11","objectID":"/gost/:8:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"访问测试 065d67d07b0a820f.gost.p-pp.cn ","date":"2024-09-11","objectID":"/gost/:9:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"参考文档 https://gost.run/blog/2023/gost-plus/ https://gost.run/tutorials/reverse-proxy-tunnel/ ","date":"2024-09-11","objectID":"/gost/:10:0","tags":[""],"title":"gost配置反向代理隧道","uri":"/gost/"},{"categories":["documentation"],"content":"服务端 ","date":"2024-09-10","objectID":"/headscale_deploy/:0:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"部署 mkdir /opt/go/src/github.com/juanfont cd /opt/go/src/github.com/juanfont git clone https://github.com/juanfont/headscale.git cd headscale mkdir config touch config/db.sqlite cp config-example.yaml config/config.yaml vim config/config.yaml docker run --name=headscale --user=0 --volume=/opt/go/src/github.com/juanfont/headscale/config:/etc/headscale/ --workdir=/ -p 8080:8080 -p 9090:9090 --restart=always --runtime=runc --detach=true headscale/headscale:v0.23.0-beta.4 serve echo \"alias headscale='docker container exec headscale headscale'\" \u003e\u003e ~/.bashrc \u0026\u0026 . ~/.bashrc ","date":"2024-09-10","objectID":"/headscale_deploy/:1:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"配置用户 headscale user create test01 客户端 ","date":"2024-09-10","objectID":"/headscale_deploy/:2:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"安装客户端 https://tailscale.com/download ","date":"2024-09-10","objectID":"/headscale_deploy/:3:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"配置客户端 tailscale login --login-server https://headscale.xxx.com ","date":"2024-09-10","objectID":"/headscale_deploy/:4:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"服务端认证客户端 headscale nodes register --user test01 --key mkey:xxxxx 参考文档 https://icloudnative.io/posts/how-to-set-up-or-migrate-headscale/ https://github.com/juanfont/headscale ","date":"2024-09-10","objectID":"/headscale_deploy/:5:0","tags":[""],"title":"部署Headscale","uri":"/headscale_deploy/"},{"categories":["documentation"],"content":"介绍 之前升级都是直接使用阿里云的控制台，只知道升级了控制面，然后升级了节点，但是不知道其中的更多细节，所以自己在测试环境升级学习下, 升级的话直接按照官方文档升级即可: https://v1-28.docs.kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrading-control-plane-nodes, 我这里主要记录下命令和遇到的问题 ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:1:0","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"命令 我的是本地测试环境，所以直接重启kubelet，如果是生产环境，请参考文档驱逐pod、配置节点不可调度 apt-get install -y kubeadm='1.28.13-1.1' kubeadm upgrade plan kubeadm upgrade apply v1.28.13 -y systemctl restart kubelet ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:2:0","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"执行upgrade apply时卡在etcd ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:3:0","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"报错 kubeadm升级的报错 \u003e etcd endpoints read from pods …… I0823 11:38:26.483010 12990 etcd.go:150] etcd endpoints read from pods: https://172.18.0.2:2379 context deadline exceeded error syncing endpoints with etcd k8s.io/kubernetes/cmd/kubeadm/app/util/etcd.NewFromCluster k8s.io/kubernetes/cmd/kubeadm/app/util/etcd/etcd.go:166 k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.StaticPodControlPlane k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade/staticpods.go:443 k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.PerformStaticPodUpgrade k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade/staticpods.go:617 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.PerformControlPlaneUpgrade k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:216 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.runApply k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:156 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.newCmdApply.func1 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:74 github.com/spf13/cobra.(*Command).execute github.com/spf13/cobra@v1.6.0/command.go:916 github.com/spf13/cobra.(*Command).ExecuteC github.com/spf13/cobra@v1.6.0/command.go:1040 github.com/spf13/cobra.(*Command).Execute github.com/spf13/cobra@v1.6.0/command.go:968 k8s.io/kubernetes/cmd/kubeadm/app.Run k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50 main.main k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25 runtime.main runtime/proc.go:271 runtime.goexit runtime/asm_amd64.s:1695 failed to create etcd client k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.StaticPodControlPlane k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade/staticpods.go:445 k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.PerformStaticPodUpgrade k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade/staticpods.go:617 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.PerformControlPlaneUpgrade k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:216 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.runApply k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:156 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.newCmdApply.func1 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:74 github.com/spf13/cobra.(*Command).execute github.com/spf13/cobra@v1.6.0/command.go:916 github.com/spf13/cobra.(*Command).ExecuteC github.com/spf13/cobra@v1.6.0/command.go:1040 github.com/spf13/cobra.(*Command).Execute github.com/spf13/cobra@v1.6.0/command.go:968 k8s.io/kubernetes/cmd/kubeadm/app.Run k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50 main.main k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25 runtime.main runtime/proc.go:271 runtime.goexit runtime/asm_amd64.s:1695 [upgrade/apply] FATAL k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.runApply k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:157 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade.newCmdApply.func1 k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade/apply.go:74 github.com/spf13/cobra.(*Command).execute github.com/spf13/cobra@v1.6.0/command.go:916 github.com/spf13/cobra.(*Command).ExecuteC github.com/spf13/cobra@v1.6.0/command.go:1040 github.com/spf13/cobra.(*Command).Execute github.com/spf13/cobra@v1.6.0/command.go:968 k8s.io/kubernetes/cmd/kubeadm/app.Run k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:50 main.main k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25 runtime.main runtime/proc.go:271 runtime.goexit runtime/asm_amd64.s:1695 etcd的logs remote error: tls: bad certificate {\"level\":\"warn\",\"ts\":\"2024-08-23T13:51:24.330935Z\",\"caller\":\"embed/config_logging.go:169\",\"msg\":\"rejected connection\",\"remote-addr\":\"172.18.0.2:41408\",\"server-name\":\"172.18.0.2\",\"error\":\"remote error: tls: bad certificate\"} 查看了相关isuee, 感觉像是etcd证书的问题: https://github.com/kubernetes/kubeadm/issues/910 https://github.com/kubernetes/kubernetes/pull/64988 https://github.com/kubernetes/kubeadm/issues/2560 ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:3:1","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"排查步骤 小版本升级 最开始看到有的isuee中说可能是跨的版本太大，所以尝试了小版本升级，结果还是报错. 在机器上安装etcdctl，然后查看etcd信息 安装etcdctl https://github.com/etcd-io/etcd/releases/tag/v3.5.7 cp /tmp/etcd-download-test/etcd /tmp/etcd-download-test/etcdctl /usr/local/bin/ etcdctl --endpoints https://127.0.0.1:2379 --cert=\"/etc/kubernetes/pki/etcd/server.crt\" --key=\"/etc/kubernetes/pki/etcd/server.key\" --cacert=\"/etc/kubernetes/pki/etcd/ca.crt\" member list -w table +------------------+---------+--------------------------+-------------------------+-------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+--------------------------+-------------------------+-------------------------+------------+ | c9d4f4d293c08711 | started | k8s-test01-control-plane | https://172.18.0.3:2380 | https://172.18.0.2:2379 | false | +------------------+---------+--------------------------+-------------------------+-------------------------+------------+ etcdctl --endpoints https://127.0.0.1:2379 --cert=\"/etc/kubernetes/pki/etcd/server.crt\" --key=\"/etc/kubernetes/pki/etcd/server.key\" --cacert=\"/etc/kubernetes/pki/etcd/ca.crt\" endpoint health -w table +------------------------+--------+-------------+-------+ | ENDPOINT | HEALTH | TOOK | ERROR | +------------------------+--------+-------------+-------+ | https://127.0.0.1:2379 | true | 11.957984ms | | +------------------------+--------+-------------+-------+ 确认etcd异常点 从上一步的结果，看到2个异常点 PEER ADDRS是172.18.0.3 (原因是因为我使用的kind安装的，重启docker后，容器的ip会变化) IS LEARNER的状态是false (这个好像是因为我是1节点的集群，所以是false) 然后当我尝试使用172.18.0.2查看etcd状态时，发现了如下报错 root@k8s-test01-control-plane:/etc/kubernetes/pki/etcd# etcdctl --endpoints https://172.18.0.2:2379 --cert=\"/etc/kubernetes/pki/etcd/server.crt\" --key=\"/etc/kubernetes/pki/etcd/server.key\" --cacert=\"/etc/kubernetes/pki/etcd/ca.crt\" m ember list -w table {\"level\":\"warn\",\"ts\":\"2024-08-23T13:32:36.628Z\",\"logger\":\"etcd-client\",\"caller\":\"v3@v3.5.7/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0003de000/172.18.0.2:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: authentication handshake failed: x509: certificate is valid for 172.18.0.3, 127.0.0.1, ::1, not 172.18.0.2\\\"\"} Error: context deadline exceeded 确认etcd证书配置的DNS openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -noout -text …… X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:9F:3B:70:71:EF:A2:A1:DA:F4:0F:87:80:D1:A0:BA:31:A5:EF:FB:3B X509v3 Subject Alternative Name: DNS:k8s-test01-control-plane, DNS:localhost, IP Address:172.18.0.3, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 …… 所以可以确认是证书的问题，因为最开始容器的IP是172.18.0.3，集群在etcd的证书中只允许了 172.18.0.3, 127.0.0.1, ::1这三个地址，所以访问172.18.0.2的时候，会报错。 ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:3:2","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"解决方法 使用cfssl重新生成支持172.18.0.2的证书即可 # 安装cfssl curl -s -L -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/{cfssl,cfssljson} # 生成证书 cd /etc/kubernetes/pki mkdir cfssl cfssl print-defaults config \u003e cfssl/ca-config.json cd /etc/kubernetes/pki/etcd echo '{\"CN\":\"k8s-test01-control-plane\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.crt -ca-key=ca.key -profile=etcd-server -hostname=\"k8s-test01-control-plane,localhost,172.18.0.3,172.18.0.2,127.0.0.1,0:0:0:0:0:0:0:1\" - | cfssljson -bare etcd-server 替换集群证书 cd /etc/kubernetes/pki/etcd mkdir server/ mv server.crt server.crt.txt server.key server/ mv etcd-server-key.pem server.key mv etcd-server.pem server.crt 重启ETCD mv /etc/kubernetes/manifests/etcd.yaml ~/etcd.yaml \u0026\u0026 sleep 3 \u0026\u0026 mv ~/etcd.yaml /etc/kubernetes/manifests/etcd.yaml 然后再次升级集群正常 ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:3:3","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"参考文档 etcdctl安装: https://github.com/etcd-io/etcd/releases/tag/v3.5.7 etcd TLS: https://etcd.io/docs/v3.5/op-guide/security/ cfssl生成证书: https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md cfssl仓库地址: https://github.com/cloudflare/cfssl.git 节点更换IP: https://www.qikqiak.com/post/how-to-change-k8s-node-ip/ ","date":"2024-08-23","objectID":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/:4:0","tags":[""],"title":"Kubernetes 1.27升级至1.28","uri":"/1.27%E5%8D%87%E7%BA%A7%E8%87%B31.28/"},{"categories":["documentation"],"content":"发版内容diff ","date":"2024-07-30","objectID":"/diff/:0:0","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"前端 https://github.com/otakustay/react-diff-view.git ","date":"2024-07-30","objectID":"/diff/:1:0","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"介绍 该工具是将git diff的结果进行解析，然后对不同的内容进行渲染，从而实现git diff的展示。 ","date":"2024-07-30","objectID":"/diff/:1:1","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"后端 https://github.com/go-git/go-git.git 该工具实现了git diff的功能，首先调用go-diff对两份数据进行diff，然后最结果进行解析，解析为git diff一样的效果 ","date":"2024-07-30","objectID":"/diff/:2:0","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"示例 对prometheus的go.mod和alertmanager的go.mod进行diff, 并在前端展示 ","date":"2024-07-30","objectID":"/diff/:3:0","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"下载文件 wget https://raw.githubusercontent.com/prometheus/prometheus/main/go.mod -O prometheus-go.mod wget https://raw.githubusercontent.com/prometheus/alertmanager/main/go.mod -O alertmanager-go.mod ","date":"2024-07-30","objectID":"/diff/:3:1","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"git diff git diff -U1 prometheus-go.mod alertmanager-go.mod ","date":"2024-07-30","objectID":"/diff/:3:2","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"使用go-git包进行diff 代码比较臃肿，直接拿的git-diff的测试代码，没优化 package main import ( \"bufio\" \"bytes\" \"fmt\" \"os\" \"testing\" \"github.com/go-git/go-git/v5/plumbing\" \"github.com/go-git/go-git/v5/plumbing/filemode\" gitdiff \"github.com/go-git/go-git/v5/plumbing/format/diff\" \"github.com/go-git/go-git/v5/utils/diff\" \"github.com/sergi/go-diff/diffmatchpatch\" ) type testFile struct { path string mode filemode.FileMode seed string } func (t testFile) Hash() plumbing.Hash { return plumbing.ComputeHash(plumbing.BlobObject, []byte(t.seed)) } func (t testFile) Mode() filemode.FileMode { return t.mode } func (t testFile) Path() string { return t.path } type testPatch struct { message string filePatches []testFilePatch } func (t testPatch) FilePatches() []gitdiff.FilePatch { var result []gitdiff.FilePatch for _, f := range t.filePatches { result = append(result, f) } return result } func (t testPatch) Message() string { return t.message } type testFilePatch struct { from, to *testFile chunks []testChunk } func (t testFilePatch) IsBinary() bool { return len(t.chunks) == 0 } func (t testFilePatch) Files() (gitdiff.File, gitdiff.File) { // Go is amazing switch { case t.from == nil \u0026\u0026 t.to == nil: return nil, nil case t.from == nil: return nil, t.to case t.to == nil: return t.from, nil } return t.from, t.to } func (t testFilePatch) Chunks() []gitdiff.Chunk { var result []gitdiff.Chunk for _, c := range t.chunks { result = append(result, c) } return result } type testChunk struct { content string op gitdiff.Operation } func (t testChunk) Content() string { return t.content } func (t testChunk) Type() gitdiff.Operation { return t.op } // readFile reads the contents of a file and returns it as a string. func readFile(filePath string) (string, error) { file, err := os.Open(filePath) if err != nil { return \"\", err } defer file.Close() scanner := bufio.NewScanner(file) var content string for scanner.Scan() { content += scanner.Text() + \"\\n\" } if err := scanner.Err(); err != nil { return \"\", err } return content, nil } func TestCustomDiff(t *testing.T) { // if len(os.Args) != 3 { // t.Logf(\"Usage: %s \u003cfile1\u003e \u003cfile2\u003e\\n\", os.Args[0]) // os.Exit(1) // } file1Path := \"prometheus-go.mod\" file2Path := \"alertmanager-go.mod\" text1, err := readFile(file1Path) if err != nil { fmt.Println(\"Error reading file1:\", err) os.Exit(1) } chunks := make([]testChunk, 0) text2, err := readFile(file2Path) if err != nil { fmt.Println(\"Error reading file2:\", err) os.Exit(1) } diffs := diff.Do(text1, text2) for _, chunk := range diffs { switch chunk.Type { case diffmatchpatch.DiffDelete: chunks = append(chunks, testChunk{ content: chunk.Text, op: gitdiff.Delete, }) default: chunks = append(chunks, testChunk{ content: chunk.Text, op: gitdiff.Operation(chunk.Type), }) } t.Log(\"------------------chunk------------------\") t.Log(chunk.Type) t.Log(chunk.Text) } buffer := bytes.NewBuffer(nil) e := gitdiff.NewUnifiedEncoder(buffer, 1).SetColor(gitdiff.NewColorConfig()) p := testPatch{ message: \"\", filePatches: []testFilePatch{{ from: \u0026testFile{ mode: filemode.Regular, path: file1Path, seed: text1, }, to: \u0026testFile{ mode: filemode.Regular, path: file2Path, seed: text2, }, chunks: chunks, }}, } err = e.Encode(p) if err != nil { t.Fatal(err) } fmt.Println(buffer.String()) } ","date":"2024-07-30","objectID":"/diff/:3:3","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"前端 import { parseDiff, Diff, Hunk } from \"react-diff-view\"; import \"react-diff-view/style/index.css\"; const diffText1 = ` DIFF的结果 `; function renderFile({ oldRevision, newRevision, type, hunks }) { return ( \u003cDiff key={oldRevision + \"-\" + newRevision} viewType=\"split\" diffType={type} hunks={hunks} \u003e {(hunks) =\u003e hunks.map((hunk) =\u003e \u003cHunk key={hunk.content} hunk={hunk} /\u003e)} \u003c/Diff\u003e ); } export default function App({}) { const files1 = parseDiff(diffText1); // console.log(files); return ( \u003cdiv\u003e \u003cdiv\u003e{files1.map(renderFile)}\u003c/div\u003e \u003c/div\u003e ); } ","date":"2024-07-30","objectID":"/diff/:3:4","tags":[""],"title":"发版内容diff","uri":"/diff/"},{"categories":["documentation"],"content":"报错 ","date":"2024-05-30","objectID":"/directory-index-full/:0:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"创建文件报错 No space left on device ","date":"2024-05-30","objectID":"/directory-index-full/:1:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"syslog日志错误 Directory (ino: 4230742) index full, reach max htree level :2 Large directory feature is not enabled on this filesystem 背景 在data目录下无法创建file-1, 但是可以创建file-2, 通过df -hT 和 df -hi确认不是磁盘不足、不是inode不足导致 经过排查是因为目录下的文件超过了目录文件限制. 但是删除一些文件后，创建file-1还是报错 话说重启解决99%的问题，所以我重启发现这是剩余的1%的问题. ","date":"2024-05-30","objectID":"/directory-index-full/:2:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"出现问题的原因 ext4自身没有做相关方面的限制 Linux 2.6.23以后引入的HTree索引做的，2-level HTree索引限定了约10-12 million（百万） Linux 4.12后largedir特性开启的话3-level限定了约6 billion（十亿） https://en.wikipedia.org/wiki/Ext4 **Unlimited number of subdirectories** ext4 does not limit the number of subdirectories in a single directory, except by the inherent size limit of the directory itself. (In ext3 a directory can have at most 32,000 subdirectories.)[17][obsolete source] To allow for larger directories and continued performance, ext4 in Linux 2.6.23 and later turns on HTree indices (a specialized version of a B-tree) by default, which allows directories up to approximately 10–12 million entries to be stored in the 2-level HTree index and 2 GB directory size limit for 4 KiB block size, depending on the filename length. In Linux 4.12 and later the large_dir feature enabled a 3-level HTree and directory sizes over 2 GB, allowing approximately 6 billion entries in a single directory. 解决方案 ","date":"2024-05-30","objectID":"/directory-index-full/:3:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"tune2fs启用磁盘large_dir 我测试过一次这种方案，当时出现了系统异常 tune2fs -O large_dir /dev/mapper/vg_test-lv_home ","date":"2024-05-30","objectID":"/directory-index-full/:4:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"重建目录 mv data data-bak mkdir data find data-bak/ -type f | xargs -I {} -n 1 mv {} data/ 参考连接 http://blog.wafcloud.cn/linux/linux-ext4-reach-max-htree-level2.html ","date":"2024-05-30","objectID":"/directory-index-full/:5:0","tags":[""],"title":"单个目录下文件爆满","uri":"/directory-index-full/"},{"categories":["documentation"],"content":"安装命令 pip install csvkit mycli 打印JSON格式 ","date":"2024-05-30","objectID":"/json-format/:0:0","tags":[""],"title":"将Mysql查询内容转换为JSON","uri":"/json-format/"},{"categories":["documentation"],"content":"在Mycli终端中打印 $ mycli -P 3306 -proot mysql MySQL root@localhost:mysql\u003e \\T csv MySQL root@localhost:mysql\u003e pager csvjson | jq '.' ","date":"2024-05-30","objectID":"/json-format/:1:0","tags":[""],"title":"将Mysql查询内容转换为JSON","uri":"/json-format/"},{"categories":["documentation"],"content":"在Linux命令行打印 mycli -P 3306 -proot mysql --csv -e \"select * from user;\" | csvjson | jq . 参考连接 https://github.com/dbcli/mycli/issues/753#issuecomment-504279817 ","date":"2024-05-30","objectID":"/json-format/:2:0","tags":[""],"title":"将Mysql查询内容转换为JSON","uri":"/json-format/"},{"categories":["documentation"],"content":"React Antd ProTable expandable.expandedRowRender支持滚动更新 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:0:0","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"需求介绍 报警平台需要展示各个报警的事件，相同的报警标题会被聚合在一起，但是会记录每次报警内容作为事件展示，有时候出现几百万次报警，导致前端加载缓慢，所以考虑报警事件使用滚动加载的方式获取 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:1:0","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"前端实现 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:2:0","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"protable \u003cProTable\u003cAPI.RuleListItem, API.PageParams\u003e actionRef={actionRef} rowKey=\"ID\" search={{ labelWidth: \"auto\", defaultCollapsed: true, }} scroll={{ x: true, }} dateFormatter={(value, valueType) =\u003e { return value.format('YYYY-MM-DDTHH:mm:ss.SSS+08:00'); }} form={{ syncToUrl: (values, type) =\u003e { if (type === 'get') { return { ...values, created_at: [values.startTime, values.endTime], }; } return values; }, }} headerTitle={getTitle()} request={async (params, sorter, filter) =\u003e { let data = await GetObjects() // 获取报警列表 return data }} pagination={{ showSizeChanger: true, }} columns={columns} rowSelection={{ selections: [ Table.SELECTION_INVERT, Table.SELECTION_NONE, ], preserveSelectedRowKeys: true, onChange: (_, selectedRows) =\u003e { setSelectedRows(selectedRows); }, }} expandable={{ expandedRowRender: record =\u003e ( \u003cExpandedRowRender record={record} /\u003e // 将record传给expandedRowRender ), }} /\u003e ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:2:1","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":" const expandedRowRender = React.memo(({ record }) =\u003e { const [expandedData, setExpandedData] = useState\u003c[]\u003e(record.events); // 首先在获取报警列表时，会拿到报警的前10条事件，初始化事件 const loadMoreData = () =\u003e { console.log(\"loadMoreData\"); GetEvents(\"events\", { alarmID: record.ID, startCount: expandedData.length, pageSize: 10 }).then((res) =\u003e { // 后端offset startCount limit pageSize setExpandedData(expandedData =\u003e { return [...expandedData, ...res.data].sort((firstItem, secondItem) =\u003e secondItem.ID - firstItem.ID) // 将已有数据和新数据进行合并并按照ID倒序展示 }) } ) }; return ( \u003cdiv id=\"scrollableDiv\" style={{ height: 300, overflow: 'auto', padding: '0 16px', border: '1px solid rgba(140, 140, 140, 0.35)', }} \u003e \u003cInfiniteScroll dataLength={expandedData.length} next={loadMoreData} // 每次滚动到末尾时，会执行loadMoreData获取更多的数据. hasMore={expandedData.length \u003c record.eventCount} // 是否有更多数据,eventCount会记录event的总数量，如果当前数据长度小于eventCount，说明还有更多事件 loader={\u003cSpin /\u003e} endMessage={\u003cDivider plain\u003eIt is all, nothing more 🤐\u003c/Divider\u003e} scrollableTarget=\"scrollableDiv\" \u003e \u003cProTable columns={EventColumns} headerTitle={false} search={false} options={false} dataSource={expandedData} pagination={false} rowKey=\"ID\" /\u003e \u003c/InfiniteScroll\u003e \u003c/div \u003e ); }); ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:2:2","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"相关问题 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:3:0","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"Rendered more fewer hooks than during the previous render 这个报错是因为我在expandedRowRender中使用useState的报错，因为expandedRowRender是属于retuen的内容，会有多次渲染，如果在expandedRowRender使用useState，会导致useState被渲染多次，这是React不允许的，参考: https://www.cnblogs.com/chuckQu/p/16644590.html. 后续经过和GPT的沟通，发现可以使用React.memo解决该问题 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:3:1","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"在使用React.memo之前，因为我的expandedRowRender是在另一个TS文件中，使用函数传递record，然后在expandedRowRender中获取新的数据，并更新主组件中的内容，但是我发现主组件内容更新后在expandedRowRender中的数据不会被重新渲染,参考的GPT代码如下 import React, { useState, useEffect } from 'react'; import { Table } from 'antd'; import ExpandedRowComponent from './ExpandedRowComponent'; const ParentComponent = () =\u003e { const [expandedDataMap, setExpandedDataMap] = useState({}); // 更新展开行的数据 const updateExpandedRowData = (recordKey, newData) =\u003e { // 创建新的对象以确保引用发生变化 const newExpandedDataMap = { ...expandedDataMap }; newExpandedDataMap[recordKey] = newData; setExpandedDataMap(newExpandedDataMap); }; useEffect(() =\u003e { const intervalId = setInterval(() =\u003e { console.log('Expanded Data Map:', expandedDataMap); }, 3000); return () =\u003e { clearInterval(intervalId); }; }, [expandedDataMap]); const columns = [ // 主表格的列定义 // 例如：{ title: 'Name', dataIndex: 'name', key: 'name' } ]; const data = [ // 主表格数据 // 例如：{ key: '1', name: 'John Brown' } ]; return ( \u003cTable columns={columns} dataSource={data} expandable={{ expandedRowRender: (record) =\u003e ( \u003cExpandedRowComponent record={record} expandedData={expandedDataMap[record.key] || []} // 传递展开行数据 updateExpandedRowData={updateExpandedRowData} /\u003e ), onExpand: (expanded, record) =\u003e { // 在展开或收起时更新状态 if (expanded) { // 加载展开的数据并更新状态 // 例如：fetchExpandedData(record.key).then(newData =\u003e updateExpandedRowData(record.key, newData)); } else { // 收起时清空展开的数据状态 updateExpandedRowData(record.key, []); } }, }} /\u003e ); }; export default ParentComponent; // ExpandedRowComponent.js import React from 'react'; import { Table } from 'antd'; const ExpandedRowComponent = ({ record, expandedData, updateExpandedRowData }) =\u003e { const handleUpdateExpandedData = () =\u003e { // 调用父组件传递的更新函数来更新展开行的数据 updateExpandedRowData(record.key, /* 新数据 */); }; const columns = [ // 定义表格列 // 例如：{ title: 'Column 1', dataIndex: 'column1', key: 'column1' } ]; return ( \u003cTable columns={columns} dataSource={expandedData} pagination={false} // 在这里添加滚动更新的逻辑 // 例如：onScroll={handleUpdateExpandedData} /\u003e ); }; export default ExpandedRowComponent; 在ExpandedRowComponent中跑了updateExpandedRowData之后，ParentComponent中每三秒打印的内容中可以看到新的数据，但是在ExpandedRowComponent中的表格并未重新渲染 ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:3:2","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"参考文档 列表滚动加载: https://ant.design/components/list-cn#list-demo-infinite-load protable树状数据如何实现懒加载子节点: https://github.com/ant-design/pro-components/issues/2332 滚动加载组件： https://github.com/ankeetmaini/react-infinite-scroll-component.git ","date":"2024-04-11","objectID":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/:4:0","tags":[""],"title":"React Antd ProTable滚动更新","uri":"/protable%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0/"},{"categories":["documentation"],"content":"部署k8s多集群管理利器Karmada ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:0:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"创建两个kubernetes集群 cat \u003c\u003cEOF \u003e cluster01-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster01 nodes: - role: control-plane networking: podSubnet: \"10.10.0.0/16\" serviceSubnet: \"10.11.0.0/16\" EOF kind create cluster --config cluster01-kind.yaml kubectl config set-cluster kind-cluster01 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane):6443 cat \u003c\u003cEOF \u003e cluster02-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster02 nodes: - role: control-plane networking: podSubnet: \"10.20.0.0/16\" serviceSubnet: \"10.21.0.0/16\" EOF kind create cluster --config cluster02-kind.yaml kubectl config set-cluster kind-cluster02 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane):6443 ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:1:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"Karmada kubectl 插件 kubectl-krew: https://krew.sigs.k8s.io/ kubectl-krew install karmada ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:2:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"通过 Karmada 命令行工具安装 Karmada kubectl karmada init --context=\"kind-cluster01\" I0219 19:40:52.380107 12365 deploy.go:245] kubeconfig file: , kubernetes: https://172.18.0.2:6443 I0219 19:40:52.410975 12365 deploy.go:265] karmada apiserver ip: [172.18.0.2] I0219 19:40:52.960643 12365 cert.go:246] Generate ca certificate success. I0219 19:40:53.186575 12365 cert.go:246] Generate karmada certificate success. I0219 19:40:53.266349 12365 cert.go:246] Generate apiserver certificate success. I0219 19:40:53.436140 12365 cert.go:246] Generate front-proxy-ca certificate success. I0219 19:40:53.525006 12365 cert.go:246] Generate front-proxy-client certificate success. I0219 19:40:53.921380 12365 cert.go:246] Generate etcd-ca certificate success. I0219 19:40:54.088352 12365 cert.go:246] Generate etcd-server certificate success. I0219 19:40:54.224864 12365 cert.go:246] Generate etcd-client certificate success. I0219 19:40:54.225086 12365 deploy.go:361] download crds file:https://github.com/karmada-io/karmada/releases/download/v1.8.1/crds.tar.gz Downloading...[ 100.00% ] Download complete. I0219 19:40:56.373735 12365 deploy.go:621] Create karmada kubeconfig success. I0219 19:40:56.383437 12365 idempotency.go:267] Namespace karmada-system has been created or updated. I0219 19:40:56.420546 12365 idempotency.go:291] Service karmada-system/etcd has been created or updated. I0219 19:40:56.420606 12365 deploy.go:427] Create etcd StatefulSets I0219 19:41:43.446937 12365 deploy.go:436] Create karmada ApiServer Deployment I0219 19:41:43.461798 12365 idempotency.go:291] Service karmada-system/karmada-apiserver has been created or updated. I0219 19:42:14.479243 12365 deploy.go:451] Create karmada aggregated apiserver Deployment I0219 19:42:14.502694 12365 idempotency.go:291] Service karmada-system/karmada-aggregated-apiserver has been created or updated. I0219 19:42:36.546334 12365 idempotency.go:267] Namespace karmada-system has been created or updated. I0219 19:42:36.546669 12365 deploy.go:85] Initialize karmada bases crd resource `/etc/karmada/crds/bases` I0219 19:42:36.548480 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.563038 12365 deploy.go:250] Create CRD cronfederatedhpas.autoscaling.karmada.io successfully. I0219 19:42:36.569360 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.599014 12365 deploy.go:250] Create CRD federatedhpas.autoscaling.karmada.io successfully. I0219 19:42:36.600462 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.611881 12365 deploy.go:250] Create CRD resourceinterpretercustomizations.config.karmada.io successfully. I0219 19:42:36.613298 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.624451 12365 deploy.go:250] Create CRD resourceinterpreterwebhookconfigurations.config.karmada.io successfully. I0219 19:42:36.628038 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.639566 12365 deploy.go:250] Create CRD serviceexports.multicluster.x-k8s.io successfully. I0219 19:42:36.640702 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.654625 12365 deploy.go:250] Create CRD serviceimports.multicluster.x-k8s.io successfully. I0219 19:42:36.658436 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.720222 12365 deploy.go:250] Create CRD multiclusteringresses.networking.karmada.io successfully. I0219 19:42:36.725982 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.762646 12365 deploy.go:250] Create CRD multiclusterservices.networking.karmada.io successfully. I0219 19:42:36.783995 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.849103 12365 deploy.go:250] Create CRD clusteroverridepolicies.policy.karmada.io successfully. I0219 19:42:36.855469 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.901329 12365 deploy.go:250] Create CRD clusterpropagationpolicies.policy.karmada.io successfully. I0219 19:42:36.903346 12365 deploy.go:240] Attempting to create CRD I0219 19:42:36.918197 12365 deploy.go:250] Create CRD federatedresourc","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:3:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"集群注册 Karmada 支持 Push 和 Pull 两种模式来管理成员集群。 Push 和 Pull 模式的主要区别在于部署清单时访问成员集群的方式。 ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:4:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"PUSH Mode Karmada 控制平面将直接访问成员集群的 kube-apiserver 以获取集群状态并部署清单。 注册 kubectl karmada join kind-cluster01 --kubeconfig=/etc/karmada/karmada-apiserver.config --cluster-kubeconfig=/root/.kube/config --cluster-context=kind-cluster01 cluster(kind-cluster01) is joined successfully kubectl karmada join kind-cluster02 --kubeconfig=/etc/karmada/karmada-apiserver.config --cluster-kubeconfig=/root/.kube/config --cluster-context=kind-cluster02 cluster(kind-cluster02) is joined successfully kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters NAME VERSION MODE READY AGE kind-cluster01 v1.27.3 Push True 16s kind-cluster02 v1.27.3 Push True 5s ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:4:1","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"Pull mode Karmada 控制平面不会访问成员集群，而是将其委托给名为 Karmada-agent 的额外组件。 每个 karmada-agent 服务于一个集群并负责： 将集群注册到 Karmada（创建 Cluster 对象） 维护集群状态并向 Karmada 报告（更新集群对象的状态） 从 Karmada 执行空间（命名空间、karmada-es-\u003c集群名称\u003e）监视清单，并将监视的资源部署到代理服务的集群。 ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:4:2","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"多集群调度 https://karmada.io/zh/docs/userguide/scheduling/resource-propagating#fieldselector 提供 PropagationPolicy 和 ClusterPropagationPolicy API 来传播资源。关于两个API之间的差异，请参见此处 ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:5:0","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"部署最简单的多集群Deployment 创建传播策略 cat \u003c\u003cEOF \u003e propagationpolicy.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: propagaetion-nginx # The default namespace is `default`. spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx # If no namespace is specified, the namespace is inherited from the parent object scope. placement: clusterAffinity: clusterNames: - kind-cluster01 EOF kubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f propagationpolicy.yaml kubectl --kubeconfig /etc/karmada/karmada-apiserver.config create deployment nginx --image nginx kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get deployment NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx kind-cluster01 1/1 1 1 3m36s Y kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-77b4fdf86c-zxjp2 kind-cluster01 1/1 Running 0 3m36s kubectl --context kind-cluster01 get pods NAME READY STATUS RESTARTS AGE nginx-77b4fdf86c-zxjp2 1/1 Running 0 2m36s kubectl --context kind-cluster02 get pods No resources found in default namespace. 更新传播策略 可以通过应用新的 YAML 文件来更新 propagationPolicy。此 YAML 文件将部署传播到 kind-cluster02 集群。 cat \u003c\u003cEOF \u003e propagationpolicy-update.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: propagaetion-nginx spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: # Modify the selected cluster to propagate the Deployment. - kind-cluster02 EOF kubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f propagationpolicy-update.yaml kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get deployment NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx kind-cluster02 1/1 1 1 112s Y kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-77b4fdf86c-jk5qm kind-cluster02 1/1 Running 0 99s kubectl --context kind-cluster02 get pods NAME READY STATUS RESTARTS AGE nginx-77b4fdf86c-jk5qm 1/1 Running 0 2m13s ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:5:1","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"将部署部署到指定的一组目标集群中 PropagationPolicy 的 .spec.placement.clusterAffinity 字段表示对特定集群集合的调度限制，没有该限制，任何集群都可以成为调度候选者。 它有四个字段可以设置： LabelSelector FieldSelector ClusterNames ExcludeClusters LabelSelector 支持matchLabels和matchExpressions， 区别可以参考支持基于集合需求的资源 配置标签 kubectl --kubeconfig /etc/karmada/karmada-apiserver.config label cluster kind-cluster01 app=cluster01 kubectl --kubeconfig /etc/karmada/karmada-apiserver.config label cluster kind-cluster02 app=cluster02 kubectl --kubeconfig /etc/karmada/karmada-apiserver.config label cluster kind-cluster01 deploy-type=kind kubectl --kubeconfig /etc/karmada/karmada-apiserver.config label cluster kind-cluster02 deploy-type=kind kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get cluster --show-labels NAME VERSION MODE READY AGE LABELS kind-cluster01 v1.27.3 Push True 10h app=cluster01,deploy-type=kind kind-cluster02 v1.27.3 Push True 10h app=cluster02,deploy-type=kind matchLabels cat \u003c\u003cEOF \u003e propagationpolicy-labelSelector-matchLabels.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: propagaetion-nginx spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: labelSelector: matchLabels: app: cluster01 EOF kubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f propagationpolicy-labelSelector-matchLabels.yaml propagationpolicy.policy.karmada.io/propagaetion-nginx configured 查看集群deployment部署情况 kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get deployments.apps NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx kind-cluster01 2/2 2 2 13s Y kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-77b4fdf86c-s7qzm kind-cluster01 1/1 Running 0 48s nginx-77b4fdf86c-v28n5 kind-cluster01 1/1 Running 0 48s matchExpressions cat \u003c\u003cEOF \u003e propagationpolicy-labelSelector-matchExpressions.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: propagaetion-nginx spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: labelSelector: matchExpressions: - key: app operator: In values: - cluster01 - cluster02 EOF kubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f propagationpolicy-labelSelector-matchExpressions.yaml propagationpolicy.policy.karmada.io/propagaetion-nginx configured 查看集群deployment部署情况 kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get deployments.apps NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx kind-cluster01 2/2 2 2 3m34s Y nginx kind-cluster02 2/2 2 2 23s Y kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-77b4fdf86c-s7qzm kind-cluster01 1/1 Running 0 3m40s nginx-77b4fdf86c-v28n5 kind-cluster01 1/1 Running 0 3m40s nginx-77b4fdf86c-dvzfx kind-cluster02 1/1 Running 0 29s nginx-77b4fdf86c-m6brd kind-cluster02 1/1 Running 0 29s FieldSelector 支持matchLabels和matchExpressions， 区别可以参考支持基于集合需求的资源 配置Field FieldSelector 支持三个值provider, region, zone, 分别对应cluster对象的.spec.provider, .spec.region, .spec.zone的选择 bectl --kubeconfig /etc/karmada/karmada-apiserver.config patch clusters.cluster.karmada.io kind-cluster01 kind-cluster02 -p '{\"spec\": {\"provider\": \"kind\"} }' cluster.cluster.karmada.io/kind-cluster01 patched cluster.cluster.karmada.io/kind-cluster02 patched kubectl --kubeconfig /etc/karmada/karmada-apiserver.config patch clusters.cluster.karmada.io kind-cluster01 kind-cluster02 -p '{\"spec\": {\"region\": \"local\"} }' cluster.cluster.karmada.io/kind-cluster01 patched cluster.cluster.karmada.io/kind-cluster02 patched kubectl --kubeconfig /etc/karmada/karmada-apiserver.config patch clusters.cluster.karmada.io kind-cluster01 -p '{\"spec\": {\"zone\": \"cluster01\"} }' cluster.cluster.karmada.io/kind-cluster01 patched kubectl --kubeconfig /etc/karmada/karmada-apiserver.config patch clusters.cluster.karmada.io kind-cluster02 -p '{\"spec\": {\"zone\": ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:5:2","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"多个集群亲和性组 ","date":"2024-02-19","objectID":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/:5:3","tags":[""],"title":"部署k8s多集群管理利器Karmada","uri":"/01%E9%83%A8%E7%BD%B2k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8karmada/"},{"categories":["documentation"],"content":"使用 WebAssembly 对 Istio 进行扩展 ","date":"2024-01-26","objectID":"/2.wasm-istio/:0:0","tags":[""],"title":"使用 WebAssembly 对 Istio 进行扩展","uri":"/2.wasm-istio/"},{"categories":["documentation"],"content":"Proxy-Wasm 编写 WASM 的工具有 Solo.io 团队的 wasme、tinygo等，目前应用比较多是 tinygo，tinygo 支持的包可以查看 https://tinygo.org/docs/reference/lang-support/stdlib/ 进行了解。 TinyGo 是 Go 编程语言规范的一个编译器实现，为什么不使用官方的 Go 编译器？目前官方编译器无法生成可以在浏览器外部运行的 WASM 二进制文件，因此也无法生成与 Proxy-Wasm 兼容的二进制文件。 ","date":"2024-01-26","objectID":"/2.wasm-istio/:1:0","tags":[""],"title":"使用 WebAssembly 对 Istio 进行扩展","uri":"/2.wasm-istio/"},{"categories":["documentation"],"content":"Proxy-Wasm Proxy-Wasm是开源社区针对「网络代理场景」设计的一套 ABI 规范，定义了网络代理和运行在网络代理内部的 Wasm 虚拟机之间的接口，属于当前的事实规范。当前支持该规范的网络代理软件包括 Envoy、MOSN 和 ATS(Apache Traffic Server)，支持该规范的 Wasm 扩展 SDK 包括 C++、Rust 和 Go。采用该规范的好处在于能让 Wasm 扩展程序在不同的网络代理产品上运行，比如 MOSN 的 Wasm 扩展程序可以运行在 Envoy 上，而 Envoy 的 Wasm 扩展程序也可以运行在 MOSN 上。 Proxy-Wasm 规范定义了宿主机与 Wasm 扩展程序之间的交互细节，包括 API 列表、函数调用规范以及数据传输规范这几个方面。其中，API 列表包含了 L4/L7、property、metrics、日志等方面的扩展点，涵盖了网络代理场景下所需的大部分交互点。目前实现该规范的 Wasm 扩展 SDK 包括 AssemblyScript、C++、Rust 和 Go： AssemblyScript SDK C++ SDK Go (TinyGo) SDK Rust SDK 为了方便，我们也直接选择已有的 proxy-wasm-go-sdk 这个 SDK 进行开发。这个 Proxy-Wasm Go SDK 是用于使用 Go 编程语言在 Proxy-Wasm ABI 规范之上扩展网络代理（例如 Envoyproxy）的 SDK，有了这个 SDK，每个人都可以轻松地生成与 Proxy-Wasm 规范兼容的 Wasm 二进制文件，而无需了解低级且对于没有专业知识的人来说难以理解的 Proxy-Wasm ABI 规范。 ","date":"2024-01-26","objectID":"/2.wasm-istio/:1:1","tags":[""],"title":"使用 WebAssembly 对 Istio 进行扩展","uri":"/2.wasm-istio/"},{"categories":["documentation"],"content":"参考文档 使用 WebAssembly 对 Istio 进行扩展 ","date":"2024-01-26","objectID":"/2.wasm-istio/:2:0","tags":[""],"title":"使用 WebAssembly 对 Istio 进行扩展","uri":"/2.wasm-istio/"},{"categories":["documentation"],"content":"编写WASM(WebAssembly)并运行 ","date":"2024-01-24","objectID":"/1.wasm-run/:0:0","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"编写WASM（WebAssembly） ","date":"2024-01-24","objectID":"/1.wasm-run/:1:0","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"GO 安装tinygo https://github.com/tinygo-org/tinygo/releases wget https://github.com/tinygo-org/tinygo/releases/download/v0.30.0/tinygo0.30.0.linux-amd64.tar.gz tar xf tinygo0.30.0.linux-amd64.tar.gz -C /usr/local/ /usr/local/tinygo/bin/tinygo version tinygo version 0.30.0 linux/amd64 (using go version go1.16.9 and LLVM version 16.0.1) 暴露一个add函数 package main func main() {} //export add func add(a, b int) int { return a + b } build为wasm tinygo build -o go.wasm -target=wasi go/wasm.go ","date":"2024-01-24","objectID":"/1.wasm-run/:1:1","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"rust 安装wasm-pack 将代码编译为 WebAssembly，并生成正确的打包以供在浏览器中使用 cargo install wasm-pack cargo install wasmer-cli cargo install wasm-bindgen-cli rustup target add wasm32-unknown-unknown rustup target add wasm32-wasi 创建一个webAssmebly包 cargo new wasi-demo 修改src/main.rs pub fn main() { } #[no_mangle] pub extern fn add(left: usize, right: usize) -\u003e usize { let sum = left + right; return sum; } 编译为wasm格式 wasm-pack build --target web ","date":"2024-01-24","objectID":"/1.wasm-run/:1:2","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"运行wasm ","date":"2024-01-24","objectID":"/1.wasm-run/:2:0","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"Go运行 编写wasmer-go运行代码 保存为sample.go文件 package main import ( \"flag\" \"fmt\" \"os\" wasmer \"github.com/wasmerio/wasmer-go/wasmer\" ) func main() { var wasmFile = flag.String(\"wasm_file\", \"go.wasm\", \"wasm file\") flag.Parse() wasmBytes, _ := os.ReadFile(*wasmFile) store := wasmer.NewStore(wasmer.NewEngine()) module, _ := wasmer.NewModule(store, wasmBytes) wasiEnv, _ := wasmer.NewWasiStateBuilder(\"wasi-program\"). // Choose according to your actual situation // Argument(\"--foo\"). // Environment(\"ABC\", \"DEF\"). // MapDirectory(\"./\", \".\"). Finalize() importObject, err := wasiEnv.GenerateImportObject(store, module) check(err) instance, err := wasmer.NewInstance(module, importObject) check(err) start, err := instance.Exports.GetWasiStartFunction() check(err) start() add, err := instance.Exports.GetFunction(\"add\") check(err) result, _ := add(1, 2) fmt.Println(result) } func check(e error) { if e != nil { panic(e) } } ","date":"2024-01-24","objectID":"/1.wasm-run/:2:1","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"运行 # 测试GO生成的wasm $ go run sample.go --wasm_file ./go.wasm 3 # 测试rust生成的wasm $ go run sample.go --wasm_file wasi-demo/target/wasm32-wasi/release/wasi-demo.wasm 3 ","date":"2024-01-24","objectID":"/1.wasm-run/:2:2","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"参考文档 se https://blog.csdn.net/weixin_47560078/article/details/130559636 https://github.com/wasmerio/wasmer-go/tree/master/examples/wasi https://www.wkwkk.com/articles/1c90cd3673398f7f.html ","date":"2024-01-24","objectID":"/1.wasm-run/:3:0","tags":[""],"title":"编写WASM(WebAssembly)并运行","uri":"/1.wasm-run/"},{"categories":["documentation"],"content":"greenvpn介绍 我平常用VPN比较少，所以找了免费的VPN使用，greenvpn的详细使用可以参考: greenvpn注册 ","date":"2024-01-21","objectID":"/core_process_is_killed/:1:0","tags":["vpn","process-monitor"],"title":"链接已断开，核心进程被其他软件杀死","uri":"/core_process_is_killed/"},{"categories":["documentation"],"content":"连接报错: 链接已断开，核心进程被其他软件杀死 结论: 在使用greenvpn之前，使用了其他VPN或在默写情况下启用了加速器，需要先将其他虚拟网卡卸载，然后再尝试连接会让安装win10-tap-9.24.2，安装后然后即可正常连接 ","date":"2024-01-21","objectID":"/core_process_is_killed/:2:0","tags":["vpn","process-monitor"],"title":"链接已断开，核心进程被其他软件杀死","uri":"/core_process_is_killed/"},{"categories":["documentation"],"content":"解决问题的步骤 前期尝试步骤 ① 关闭安全软件，禁用相关安全软件开机启动 ② 关闭防火墙 ③ 重装greenvpn …… 解决步骤 ① 经过和ChatGPT的多次沟通，chatgpt给我提供了一个抓取进程事件的软件， Process Monitor chatgpt回答 ② 通过微软官方下载process monitor： https://learn.microsoft.com/zh-cn/sysinternals/downloads/procmon ③ 新增过滤器，过滤 greenvpn相关的进程 进程监控步骤 ④ 再次尝试连接，失败后查看process monitor抓到的进程 进程监控信息 ④ 通过设备管理器将虚拟网卡卸载 同时按下键盘上的 Windows 和 R 键, 在运行对话框中输入 devmgmt.msc 并按下 Enter 键 打开设备管理器，然后打开网络适配器 进程监控信息 ⑤ 再次连接的时候，会提示没有虚拟网卡，然后点击重新加载，然后进行win10-tap-9.24.2的安装, 安装完成后即可正常连接vpn greenvpn虚拟模块重新加载 ","date":"2024-01-21","objectID":"/core_process_is_killed/:2:1","tags":["vpn","process-monitor"],"title":"链接已断开，核心进程被其他软件杀死","uri":"/core_process_is_killed/"},{"categories":["documentation"],"content":"GreenVPN注册 一、greenvpn下载 官网地址: GreenVPN - Free VPN Service | Free VPN Software 二、注册greenvpn新账户 ","date":"2024-01-20","objectID":"/greenvpn-register/:0:0","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"2.1 获取临时邮箱 临时邮箱获取地址：更改电子邮件地址 - Temp Mail (temp-mail.org) 打开页面后，复制邮件，然后点击刷新（会等待新邮件的发送） ","date":"2024-01-20","objectID":"/greenvpn-register/:1:0","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"2.2 注册greenvpn ","date":"2024-01-20","objectID":"/greenvpn-register/:2:0","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"通过web页面注册 打开地址: GreenVPN - Free VPN Service | Free VPN Software输入邮件注册即可 ","date":"2024-01-20","objectID":"/greenvpn-register/:2:1","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"通过greenvpn客户端注册 安装后点击用户登录/注册账户，然后点击注册账户即可 ","date":"2024-01-20","objectID":"/greenvpn-register/:2:2","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"使用powershell终端命令注册 ①打开终端 1）点击桌面下方的Windows按钮或者俺键盘的Win键 2）搜索powershell打开 ②注册账户 curl.exe -H \"User-Agent:Mozilla/5.0\" -H \"Content-Type:application/x-www-form-urlencoded\" -X POST \"https://www.wzjsq.xyz/regist.shtml?target=\u0026userName=jasadi1014@ibtrades.com\u0026password1=qazwsx123123\u0026password2=qazwsx123123\u0026device=web\u0026identifier=web\u0026register_submit=Sign+up\" userName=jasadi1014@ibtrades.com：邮箱地址 password1=qazwsx123123：密码 password2=qazwsx123123：密码 ","date":"2024-01-20","objectID":"/greenvpn-register/:2:3","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"注册1GB流量的账户 默认注册的邮箱是200MB或者500MB, 如果要注册1GB流量的账号，需要使用非中国大陆的IP地址. 所以可以先注册一个正常的账户，然后连接VPN后通过非大陆的IP再次进行账号注册，即可注册1GB流量的账户 ① 连接VPN后使用WEB页面注册 ② 使用powershell命令行时，增加VPN代理 -x 是指定代理的命令，greenvpn会默认监听8080作为代理端口 curl.exe -x 127.0.0.1:8080 -H \"User-Agent:Mozilla/5.0\" -H \"Content-Type:application/x-www-form-urlencoded\" -X POST \"https://www.wzjsq.xyz/regist.shtml?target=\u0026userName=jasadi1014@ibtrades.com\u0026password1=qazwsx123123\u0026password2=qazwsx123123\u0026device=web\u0026identifier=web\u0026register_submit=Sign+up\" ","date":"2024-01-20","objectID":"/greenvpn-register/:2:4","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"greenvpn的加速类型有\"网页/视频\", “软件/游戏” 网页/视频 原理是在\"internet属性–\u003e连接–\u003e局域网设置\"中增加一个代理服务器， 局域网设置-代理服务器 软件/游戏 这个就是一个网卡级别的VPN了，除了浏览器，其他软件访问公网的时候也走VPN ","date":"2024-01-20","objectID":"/greenvpn-register/:2:5","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"ApiFox自动化邮件注册 xuan.dong 在 Apifox 邀请你加入项目 temp-mail https://app.apifox.com/invite/project?token=0W6Td6HySKUkqUJPeeesP ","date":"2024-01-20","objectID":"/greenvpn-register/:3:0","tags":["vpn"],"title":"GreenVPN注册","uri":"/greenvpn-register/"},{"categories":["documentation"],"content":"一、istio多集群模型介绍 Istio 多集群网格有多种模型，在网络拓扑上分为扁平网络和非扁平网络，在控制面上分为单一控制平面和多控制平面。 Istio 多集群网格有多种模型，在网络拓扑上分为扁平网络和非扁平网络，在控制面上分为单一控制平面和多控制平面。 扁平网络：所有集群都在同一个网络中，可以直接访问到其他集群的服务，不需要通过网关。 优点： 跨集群访问不经过东西向网关，延迟低 缺点：组网较为复杂，Service、Pod 网段不能重叠，借助 VPN 等技术将所有集群的 Pod 网络打通，需要考虑网络安全问题 非扁平网络：集群之间的网络是隔离的，需要通过网关访问其他集群的服务。 优点：不同集群的网络是互相隔离的，安全性更高，不需要打通不同集群的容器网络，不用提前规划集群的网段 缺点：跨集群访问依赖东西向网关，延迟高。东西向网关工作模式是 TLS AUTO_PASSTHROUGH，不支持 HTTP 路由策略。 单控制面：所有集群共用一个控制平面，所有集群的配置都在同一个控制平面中。 优点：所有集群的配置都在同一个控制平面中，集群之间的配置可以共享，部署运维更简单 缺点：控制平面的性能和可用性会受到影响，不适合大规模集群 多控制面：每个集群都有一个独立的控制平面，集群之间的配置不共享。 优点：控制平面的性能和可用性不会受到影响，适合大规模集群 缺点：集群之间的配置不共享，部署运维较为复杂 总体来说 Istio 目前支持 4 种多集群模型：扁平网络单控制面(主从架构)、扁平网络多控制面(多主架构)、非扁平网络单控制面(跨网络主从架构)、非扁平网络多控制面(跨网络多主架构)。其中扁平网络单控制面是最简单的模型，非扁平网络多控制面是最复杂的模型。 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:1:0","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"二、准备工作 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:0","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"创建两个集群 cluster01 cat \u003c\u003cEOF \u003e cluster01-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster01 nodes: - role: control-plane networking: podSubnet: \"10.10.0.0/16\" serviceSubnet: \"10.11.0.0/16\" EOF kind create cluster --config cluster01-kind.yaml # kind默认在kubeconfig中生成的地址是https://127.0.0.1:xxxxx，需要把地址改为容器的IP地址，否则两个集群无法访问 kubectl config set-cluster kind-cluster01 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane):6443 kubectl --context kind-cluster01 apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml # 等待pod ready sleep 10 \u0026\u0026 kubectl --context kind-cluster01 -n metallb-system wait --for=condition=Ready pods -l app=metallb --timeout=600s cat \u003c\u003cEOF | kubectl --context kind-cluster01 create -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: custom-172.168.11.0-255 namespace: metallb-system spec: addresses: - 172.18.11.0-172.18.11.255 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: empty namespace: metallb-system EOF cluster02 cat \u003c\u003cEOF \u003e cluster02-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster02 nodes: - role: control-plane networking: podSubnet: \"10.20.0.0/16\" serviceSubnet: \"10.21.0.0/16\" EOF kind create cluster --config cluster02-kind.yaml kubectl config set-cluster kind-cluster02 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane):6443 kubectl --context kind-cluster02 apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml sleep 10 \u0026\u0026 kubectl --context kind-cluster02 -n metallb-system wait --for=condition=Ready pods -l app=metallb --timeout=600s cat \u003c\u003cEOF | kubectl create --context kind-cluster02 -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: custom-172.168.12.0-255 namespace: metallb-system spec: addresses: - 172.18.12.0-172.18.12.255 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: empty namespace: metallb-system EOF ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:1","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"配置信任关系 默认情况下，Istio CA 会生成一个自签名的根证书和密钥，并使用它们来签署工作负载证书。 为了保护根 CA 密钥，您应该使用在安全机器上离线运行的根 CA，并使用根 CA 向运行在每个集群上的 Istio CA 签发中间证书。Istio CA 可以使用管理员指定的证书和密钥来签署工作负载证书， 并将管理员指定的根证书作为信任根分配给工作负载。 下图展示了在包含两个集群的网格中推荐的 CA 层次结构。 CA层次结构 Makefile.k8s.mk：基于 k8s 集群中的 root-ca 创建证书。默认 kubeconfig 中的当前上下文用于访问集群。 Makefile.selfsigned.mk：基于生成的自签名根创建证书。 下表描述了两个 Makefile 支持的目标 Make Target Makefile Description root-ca Makefile.selfsigned.mk 生成自签名根 CA 密钥和证书. fetch-root-ca Makefile.k8s.mk 使用默认 kubeconfig 中的当前上下文从 Kubernetes 集群获取 Istio CA. $NAME-cacerts Both 为具有 $NAME 的集群或虚拟机（例如 us-east、cluster01 等）生成由根 CA 签名的中间证书。它们存储在 $NAME 目录下。为了区分集群，我们在证书主题字段中包含位置 (L) 名称以及集群名称。 $NAMESPACE-certs Both 使用根证书为使用 serviceAccount $SERVICE_ACCOUNT 连接到命名空间 $NAMESPACE 的虚拟机生成中间证书和签名证书，并将它们存储在 $NAMESPACE 目录下。 clean Both 删除任何生成的根证书、密钥和中间文件。. 创建一个目录来存放证书和密钥 如果您计划仅部署一个主集群（即采用本地——远程部署的方式），您将只有一个 CA （即使用 cluster01 上的 istiod ）为两个集群颁发证书。 在这种情况下，您可以跳过以下 CA 证书生成步骤， 并且只需使用默认自签名的 CA 进行安装。 mkdir -p certs pushd certs 生成根CA证书 make -f ../tools/certs/Makefile.selfsigned.mk root-ca 将会生成以下文件： root-cert.pem：生成的根证书 root-key.pem：生成的根密钥 root-ca.conf：生成根证书的 openssl 配置 root-cert.csr：为根证书生成的 CSR 为cluster生成证书 make -f ../tools/certs/Makefile.selfsigned.mk cluster01-cacerts make -f ../tools/certs/Makefile.selfsigned.mk cluster02-cacerts 运行以上命令，将会在名为 cluster01、cluster02 的目录下生成以下文件： ca-cert.pem：生成的中间证书 ca-key.pem：生成的中间密钥 cert-chain.pem：istiod 使用的生成的证书链 root-cert.pem：根证书 创建cacerts Secret kubectl --context kind-cluster01 create namespace istio-system kubectl --context kind-cluster01 create secret generic cacerts -n istio-system \\ --from-file=cluster01/ca-cert.pem \\ --from-file=cluster01/ca-key.pem \\ --from-file=cluster01/root-cert.pem \\ --from-file=cluster01/cert-chain.pem kubectl --context kind-cluster02 create namespace istio-system kubectl --context kind-cluster02 create secret generic cacerts -n istio-system \\ --from-file=cluster02/ca-cert.pem \\ --from-file=cluster02/ca-key.pem \\ --from-file=cluster02/root-cert.pem \\ --from-file=cluster02/cert-chain.pem ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:2","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"三、istio多集群部署 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:0","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"扁平网络单控制面(主从架构) https://istio.io/latest/zh/docs/setup/install/multicluster/primary-remote/ 该模型下只需要将 Istio 控制面组件部署在主集群中，然后可以通过这个控制面来管理所有集群的 Service 和 Endpoint，其他的 Istio 相关的 API 比如 VirtualService、DestinationRule 等也只需要在主集群中配置即可，其他集群不需要部署 Istio 控制面组件。 控制平面的 Istiod 核心组件负责连接所有集群的 kube-apiserver，获取每个集群的 Service、Endpoint、Pod 等信息，所有集群的 Sidecar 均连接到这个中心控制面，由这个中心控制面负责所有的 Envoy Sidecar 的配置生成和分发。 主从架构的安装 多集群扁平网络模型和单一集群的服务网格在访问方式上几乎没什么区别，但是需要注意不同集群的 Service IP 和 Pod 的 IP 不能重叠，否则会导致集群之间的服务发现出现问题，这也是扁平网络模型的一个缺点，需要提前规划好集群的网段。 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --context=\"kind-cluster01\" -f cluster01.yaml -y 需要注意的是，当 values.pilot.env.EXTERNAL_ISTIOD 被设置为 true 时， 安装在 cluster01 上的控制平面也可以作为其他从集群的外部控制平面。 当这个功能被启用时，istiod 将试图获得领导权锁，并因此管理将附加到它的并且带有 适当注解的从集群 （本例中为 cluster02）。 在 cluster01 安装东西向网关 在 cluster01 中安装东西向流量专用网关，默认情况下，此网关将被公开到互联网上。 生产环境可能需要增加额外的准入限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云供应商，了解可用的选项。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster01 --network network1 | istioctl --context=kind-cluster01 install -y -f - 如果控制面已经安装了一个修订版，可在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关获取外部 IP 地址： kubectl --context=kind-cluster01 get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.96.164.163 172.18.11.1 15021:30169/TCP,15443:32267/TCP,15012:30328/TCP,15017:30664/TCP 2m7s 在 cluster01 中开放控制平面 在安装 cluster02 之前，我们需要开放 cluster01 的控制平面， 以便 cluster02 中的服务能访问到服务发现： kubectl apply --context=kind-cluster01 -f /root/istio-1.18.2/samples/multicluster/expose-istiod.yaml 设置集群 cluster02 的控制平面 我们需要通过为 istio-system 命名空间添加注解来识别应管理集群 cluster02 的外部控制平面： kubectl --context kind-cluster02 create namespace istio-system kubectl --context=\"kind-cluster02\" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster01 将 cluster02 设为从集群 保存 cluster01 东西向网关的地址。 export DISCOVERY_ADDRESS=$(kubectl --context=\"kind-cluster01\" -n istio-system get svc istio-eastwestgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 现在，为 cluster02 创建一个从集群配置： cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERV","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:1","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"扁平网络多控制面(多主架构) https://istio.io/latest/zh/docs/setup/install/multicluster/multi-primary/ 同一网络的多主集群 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置文件应用到 cluster01： istioctl install --context=\"kind-cluster01\" -f cluster01.yaml -y 将 cluster02 设为主集群 cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster02 network: network1 logAsJson: true EOF 将配置文件应用到 cluster02： istioctl install --context=\"kind-cluster02\" -f cluster02.yaml -y 开启端点发现 在 cluster02 中安装从集群的 secret，该 secret 提供 cluster01 的 API 服务器的访问权限。 istioctl create-remote-secret \\ --context=\"kind-cluster01\" \\ --name=cluster01 | \\ kubectl apply -f - --context=\"kind-cluster02\" 在 cluster01 中安装从集群的 secret，该 secret 提供 cluster02 的 API 服务器的访问权限。 istioctl create-remote-secret \\ --context=\"kind-cluster02\" \\ --name=cluster02 | \\ kubectl apply -f - --context=\"kind-cluster01\" 两个集群网络打通 docker container exec cluster01-control-plane ip route add 10.20.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane) dev eth0 docker container exec cluster01-control-plane ip route add 10.21.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane) dev eth0 docker container exec cluster02-control-plane ip route add 10.10.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane) dev eth0 docker container exec cluster02-control-plane ip route add 10.11.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane) dev eth0 后续步骤 现在，您可以验证此次安装 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:2","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"非扁平网络单控制面(跨网络主从架构) https://istio.io/latest/zh/docs/setup/install/multicluster/primary-remote_multi-network 跨网络的主从集群 为 cluster01 设置默认网络 kubectl --context=\"kind-cluster01\" label namespace istio-system topology.istio.io/network=network1 将 cluster01 设为主集群 为 cluster01 创建 Istio 配置： cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置应用到 cluster01： istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --context=\"kind-cluster01\" -f cluster01.yaml -y 请注意，values.pilot.env.EXTERNAL_ISTIOD 设置为 true。 这将启用安装在 cluster01 上的控制平面，使其也用作其他从集群的外部控制平面。 启用此特性后，istiod 将尝试获取领导选举锁， 并因此管理适当注解的且将接入的从集群（此处为 cluster02）。 在 cluster01 安装东西向网关 在 cluster01 安装专用的东西向流量网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要额外的访问限制（即通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh \\ --mesh mesh1 --cluster cluster01 --network network1 | \\ istioctl --context=\"kind-cluster01\" install -y -f - 如果控制平面已随着版本修正一起安装，请在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关获取外部 IP 地址： $ kubectl --context=kind-cluster01 get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.11.43.67 172.18.11.1 15021:31333/TCP,15443:32098/TCP,15012:31996/TCP,15017:30454/TCP 40s 开放 cluster01 控制平面 安装 cluster02 之前，我们需要先开放 cluster01 的控制平面， 以便 cluster02 中的服务能访问服务发现。 kubectl apply --context=\"kind-cluster01\" -n istio-system -f /root/istio-1.18.2/samples/multicluster/expose-istiod.yaml 为 cluster02 设置控制平面集群 命名空间 istio-system 创建之后，我们需要设置集群的网络： 我们需要通过为 istio-system 命名空间添加注解来识别应管理 cluster02 的外部控制平面集群： kubectl --context=\"kind-cluster02\" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster01 将 topology.istio.io/controlPlaneClusters 命名空间注解设置为 cluster01 将指示运行在 cluster01 上的相同命名空间（本例中为 istio-system）中的 istiod 管理作为从集群接入的 cluster02。 为 cluster02 设置默认网络 通过向 istio-system 命名空间添加标签来设置 cluster02 的网络： kubectl --context=\"kind-cluster02\" label namespace istio-system topology.istio.io/network=network2 将 cluster02 设为从集群 保存 cluster01 东西向网关的地址。 export DISCOVERY_ADDRESS=$(kubectl \\ --context=\"kind-cluster01\" \\ -n istio-system get svc istio-eastwestgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 现在，为 cluster02 创建一个从集群配置： cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUE","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:3","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"非扁平网络多控制面(跨网络多主架构) 跨网络的多主集群 为 cluster01 设置缺省网络 kubectl --context=\"kind-cluster01\" label namespace istio-system topology.istio.io/network=network1 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置文件应用到 cluster01： istioctl install --context=\"kind-cluster01\" -f cluster01.yaml -y 在 cluster01 安装东西向网关 在 cluster01 安装专用的 东西向网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要添加额外的访问限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster01 --network network1 | istioctl --context=kind-cluster01 install -y -f - 如果控制面已经安装了一个修订版，可以在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关被分配外部 IP 地址： kubectl --context=\"kind-cluster01\" get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.96.135.178 172.18.11.1 15021:31259/TCP,15443:30115/TCP,15012:30290/TCP,15017:30995/TCP 61s 开放 cluster01 中的服务 因为集群位于不同的网络中，所以我们需要在两个集群东西向网关上开放所有服务（*.local）。 虽然此网关在互联网上是公开的，但它背后的服务只能被拥有可信 mTLS 证书、工作负载 ID 的服务访问， 就像它们处于同一网络一样。 kubectl --context=\"kind-cluster01\" apply -n istio-system -f /root/istio-1.18.2/samples/multicluster/expose-services.yaml 为 cluster02 设置缺省网络 kubectl --context=\"kind-cluster02\" label namespace istio-system topology.istio.io/network=network2 将 cluster02 设为主集群 cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster02 network: network2 logAsJson: true EOF 将配置文件应用到 cluster02： istioctl install --context=\"kind-cluster02\" -f cluster02.yaml -y 在 cluster02 安装东西向网关 在 cluster02 安装专用的 东西向网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要添加额外的访问限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:4","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"验证安装结果 我们将在 cluster01 安装 V1 版的 HelloWorld 应用程序， 在 cluster02 安装 V2 版的 HelloWorld 应用程序。 当处理一个请求时，HelloWorld 会在响应消息中包含它自身的版本号。 我们也会在两个集群中均部署 Sleep 容器。 这些 Pod 将被用作客户端（source），发送请求给 HelloWorld。 最后，通过收集这些流量数据，我们将能观测并识别出是那个集群处理了请求。 部署服务 HelloWorld 为了支持从任意集群中调用 HelloWorld 服务，每个集群的 DNS 解析必须可用 （详细信息，参见部署模型 7）。 我们通过在网格的每一个集群中部署 HelloWorld 服务，来解决这个问题， 首先，在每个集群中创建命名空间 sample： kubectl create --context=\"kind-cluster01\" namespace sample kubectl create --context=\"kind-cluster02\" namespace sample 为命名空间 sample 开启 sidecar 自动注入： kubectl label --context=\"kind-cluster01\" namespace sample istio-injection=enabled kubectl label --context=\"kind-cluster02\" namespace sample istio-injection=enabled 在每个集群中创建 HelloWorld 服务： kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l service=helloworld -n sample kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l service=helloworld -n sample 部署 V1 版的 HelloWorld kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l version=v1 -n sample 部署 V2 版的 HelloWorld 把应用 helloworld-v2 部署到 cluster02： kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l version=v2 -n sample 部署 Sleep 把应用 Sleep 部署到每个集群： kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/sleep/sleep.yaml -n sample kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/sleep/sleep.yaml -n sample 等待POD启动成本 kubectl --context kind-cluster01 -n sample wait --for=condition=Ready pods -l topology.istio.io/network=network1 --timeout=600s kubectl --context kind-cluster02 -n sample wait --for=condition=Ready pods -l topology.istio.io/network=network2 --timeout=600s 验证跨集群流量 要验证跨集群负载均衡是否按预期工作，需要用 Sleep pod 重复调用服务 HelloWorld。 为了确认负载均衡按预期工作，需要从所有集群调用服务 HelloWorld。 从 cluster01 中的 Sleep pod 发送请求给服务 HelloWorld, 重复几次这个请求，验证 HelloWorld 的版本在 v1 和 v2 之间切换： for i in $(seq 10);do kubectl exec --context=\"kind-cluster01\" -n sample -c sleep \"$(kubectl get pod --context=\"kind-cluster01\" -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}')\" -- curl -s helloworld.sample:5000/hello;done Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz 现在，用 cluster02 中的 Sleep pod 重复此过程，重复几次这个请求，验证 HelloWorld 的版本在 v1 和 v2 之间切换： for i in $(seq 10);do kubectl exec --context=\"kind-cluster02\" -n sample -c sleep \"$(kubectl get pod --context=\"kind-cluster02\" -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}')\" -- curl -s helloworld.sample:5000/hello;done Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:5","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"参考文档 官方文档: 多集群安装 k8s技术圈：Istio多集群实践 metallb: Installation By Manifest ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:4:0","tags":["kubernetes","istio"],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 本文摘自 istio 学习笔记 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:0:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"背景 Istio 使用 Envoy 作为数据面转发 HTTP 请求，而 Envoy 默认要求使用 HTTP/1.1 或 HTTP/2，当客户端使用 HTTP/1.0 时就会返回 426 Upgrade Required。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:1:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"常见的 nginx 场景 如果用 nginx 进行 proxy_pass 反向代理，默认会用 HTTP/1.0，你可以显示指定 proxy_http_version 为 1.1: upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:2:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"压测场景 ab 压测时会发送 HTTP/1.0 的请求，Envoy 固定返回 426 Upgrade Required，根本不会进行转发，所以压测的结果也不会准确。可以换成其它压测工具，如 wrk 。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:3:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"让 istio 支持 HTTP/1.0 有些 SDK 或框架可能会使用 HTTP/1.0 协议，比如使用 HTTP/1.0 去资源中心 / 配置中心 拉取配置信息，在不想改动代码的情况下让服务跑在 istio 上，也可以修改 istiod 配置，加上 PILOT_HTTP10: 1 的环境变量来启用 HTTP/1.0。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:4:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"参考资料 Envoy won’t connect to my HTTP/1.0 service ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:5:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 www.jianshu.com 本文结合域名请求慢的问题，从虚拟网络定位到域名解析，根据 coredns 添加域名后缀的机制，定位 coredns 解析慢的根因。 问题现象 ======= 背景：项目是微服务 + flink，其中 flink 采用 k8s session standalone 的部署模式。 问题：微服务通过 flink restful api 启动作业的平均时长超过 40s，导致客户端超时和作业失联。 #为了排除 flink 内部启动作业耗时因素，使用 /jobs/overview 测试微服务到 flink 的网络耗时 $time curl http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview {\"jobs\":[{\"jid\":\"0bf232afb7febbb8b070221833ddb99c\",\"name\":\"TopSpeedWindowing\",\"state\":\"RUNNING\",\"start-time\":1631687894914,\"end-time\":-1,\"duration\":701488855,\"last-modification\":1631687895895,\"tasks\":{\"total\":3,\"created\":0,\"scheduled\":0,\"deploying\":0,\"running\":3,\"finished\":0,\"canceling\":0,\"canceled\":0,\"failed\":0,\"reconciling\":0}}]} real 0m10.528s user 0m0.004s sys 0m0.005s 根因定位 ======= 定位思路：理解 Service 【K8s 精选】Kubernetes Service 介绍。在使用 service name 即域名的场景下，Client Pod3 首先拿着域名去 coredns 解析成 ClusterIP，接着去请求 ClusterIP，最后通过 Kube-Proxy 把请求转发到目标后端 Pod。 Kubernetes 服务发现架构. JPG ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:0:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.1 虚拟化网络分析 2.1.1 curl 命令详解 参考 curl 命令详解，例如 curl --header \"Content-Type: application/json\" -X POST --data '{\"text\":\"germany\"}' https://labs.tib.eu/falcon/api?mode=short 2.1.2 curl 获取 http 各阶段的响应时间 参考通过 curl 得到 http 各阶段的响应时间 ① time_namelookup：DNS 解析时间 ② time_connect：连接时间，从请求开始到 DNS 解析完毕所用时间。单纯的连接时间 = time_connect - time_namelookup ③ time_appconnect：建立完成时间，例如 SSL/SSH 等建立连接或者完成三次握手的时间。 ④ time_redirect：重定向时间，包括最后一次传输前的几次重定向的 DNS 解析、连接、预传输、传输时间。 ⑤ time_pretransfer： 从开始到准备传输的时间。 ⑥ time_starttransfer：开始传输时间。在 client 发出请求后，服务端返回数据的第一个字节所用的时间。 进入业务容器，编辑完获取数据的格式后，执行 curl 命令。 /dev/null 表示空设备，即丢弃一切写入的数据，但显示写入操作成功。 $vim curl-format.txt time_namelookup: %{time_namelookup}\\n time_connect: %{time_connect}\\n time_appconnect: %{time_appconnect}\\n time_redirect: %{time_redirect}\\n time_pretransfer: %{time_pretransfer}\\n time_starttransfer: %{time_starttransfer}\\n ----------\\n time_total: %{time_total}\\n $kubectl get svc |grep flink flink-jobmanager ClusterIP 10.96.0.123 \u003cnone\u003e 8123/TCP,8124/TCP,8091/TCP 4d22h # 首先使用 ClusterIP 测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://10.96.0.123:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 81865 0 --:--:-- --:--:-- --:--:-- 94285 time_namelookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 # 然后使用 service name 即域名测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 62 0 0:00:10 0:00:10 --:--:-- 164 time_melookup: 10.516 time_connect: 10.517 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 10.517 time_starttransfer: 10.520 ---------- time_total: 10.520 对比 ClusterIP 和 service name 的接口调用时长，由 time_namelookup 可知 DNS 解析时间长。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:1:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2 域名解析分析 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.1 外部分析 - coredns 解析域名 $kubectl logs coredns-66509f5cf2-km1q4 -nkube-system 2021-09-23T01:54:04.590Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:32960-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:54:09.592Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:59978-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:00.609Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:41797-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:02.610Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:48375-\u003e100.79.1.250:53: i/o timeout 由 coredns 后台关键日志 A: read udp xxx-\u003exxx: i/o timeout 可知 IPV4 解析超时，AAAA: read udp xxx-\u003exxx: i/o timeout 可知 IPV6 解析也超时。 IPV4 和 IPV6 耗时对比 # IPV4 请求耗时 $curl -4 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 time_melookup: 0.000 time_connect: 0.000 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.000 time_starttransfer: 0.000 ---------- time_total: 3.510 # IPV6 请求耗时 $curl -6 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 146 0 0:00:04 0:00:04 --:--:-- 146 time_melookup: 4.511 time_connect: 4.512 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 4.512 time_starttransfer: 4.515 ---------- time_total: 4.515 结论：IPV6 解析比 IPV4 多耗时约 20%，说明 IPV6 对域名解析有一定的影响，建议 coredns 关闭 IPV6 解析。然而直接用 IPV4 解析也耗时 3s+，需要进一步对容器内部进行抓包分析。 建议：如果 IPV6 模式没有使用，可以关闭。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:1","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.2 内部分析 - 容器内部解析域名 通过宿主机抓起 Pod 网络数据报 nsenter 可以进入 Pod 容器 net 命名空间，同时提供一个快速进入 Pod 容器 net 命名空间脚本，可以参考在容器环境使用 tcpdump 抓包 # 步骤1 获取容器的 pid $ docker ps |grep flink-jobmanager-7b58565dc8-msgpp 1386ce6244ae 192.168.31.37:5000/flink \"bash -cx /opt/flink/s…\" 3 weeks ago Up 3 weeks k8s_flink-jobmanager-7b58565dc8-msgpp_4b6d15d8-7b54-41fb-bf46-ffa91aa33963_0 $ docker inspect 1386ce6244ae| grep Pid \"Pid\": 63046, \"PidMode\": \"\", \"PidsLimit\": 0, # 步骤2 根据 pid 执行命令 # 53 是 corends 域名解析端口 $ nsenter -t 63046 -n $ tcpdump 'src host 10.244.3.81 and src port 8091' # 步骤3 打开新窗口，进入容器并执行上述的 curl 命令 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" 下载 tcpdump 的抓包数据 xxx.pcap， 利用 wireshark 分析 tcpdump 报文，其结果如下： ① flink-jobmanager.default.svc.cluster.local 域名解析成 ip 的时间约 10s ② 在域名解析的过程中，在 flink-jobmanager.default.svc.cluster.local 的基础上，其后缀依次添加 default.svc.cluster.local、svc.cluster.local、cluster.local、openstacklocal 查看业务容器中的配置 /etc/resolv.conf，发现上述的后缀恰好是 search 内容。 $ cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen 3 容器 /etc/resolv.conf 配置分析 容器 /etc/resolv.conf 配置如下： nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:2","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"① nameserver resolv.conf 文件的第一行是 nameserver，内容是 coredns 的 ClusterIP $kubectl get svc -nkube-system |grep dns kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP 167d ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:3","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"② search resolv.conf 文件的第一行是 search。当域名解析的时候，将域名依次添加后缀，例如： flink-jobmanager.default.svc.cluster.local.default.svc.cluster.local flink-jobmanager.default.svc.cluster.local.svc.cluster.local flink-jobmanager.default.svc.cluster.local.cluster.local flink-jobmanager.default.svc.cluster.local.openstacklocal ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:4","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"③ options resolv.conf 文件的第一行是 options 其它项，常见配置是 ndots。ndots: 5 表示如果域名包含的 “.” 少于 5 个，则先添加 search 后缀，再使用绝对域名；如果域名包含的 “.” 大于等于 5 个，则先使用绝对域名，再添加 search 后缀。 #样例1 域名a.b.c.d.e，则域名的顺序如下 a.b.c.d.e.default.svc.cluster.local a.b.c.d.e.svc.cluster.local a.b.c.d.e.cluster.local a.b.c.d.e.openstacklocal a.b.c.d.e #样例2 域名a.b.c.d.e.f，则域名的顺序如下 a.b.c.d.e.f a.b.c.d.e.f.default.svc.cluster.local a.b.c.d.e.f.svc.cluster.local a.b.c.d.e.f.cluster.local a.b.c.d.e.f.openstacklocal 结合 flink-jobmanager.default.svc.cluster.local 域名解析慢的问题，由于该域名包含 “.” 等于 4，即少于 5，所以会首先依次添加后缀 default.svc.cluster.local、svc.cluster.local、cluster.local openstacklocal。因此，解决方案有 ①flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager；②ndots: 5 修改为 ndots: 4。 4 解决方案 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:5","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.1 使用简洁域名 将访问的域名从 flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager，效果如下： curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:6","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.2 resolv.conf 配置更改 将 /etc/resolv.conf 的 options 配置 ndots: 5 修改为 ndots: 4，或者修改 deployment 的 yaml 配置，效果如下： # 修改 deployment 的 yaml 配置 # spec.spec.dnsConfig.options[].ndots dnsConfig: options: - name: ndots value: 4 - name: single-request-reopen curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.005 time_connect: 0.006 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:7","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"Nginx 502问题排查 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:0:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"nginx快速定位异常 错误信息 错误说明 “upstream prematurely（过早的） closed connection” 请求uri的时候出现的异常，是由于upstream还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略 “recv() failed (104: Connection reset by peer)” （1）服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉； （2）客户关掉了浏览器，而服务器还在给客户端发送数据； （3）浏览器端按了Stop “(111: Connection refused) while connecting to upstream” 用户在连接时，若遇到后端upstream挂掉或者不通，会收到该错误 “(111: Connection refused) while reading response header from upstream” 用户在连接成功后读取数据时，若遇到后端upstrream挂掉或者不通，会收到该错误 “(111: Connection refused) while sending request to upstream” Nginx和upstream连接成功后发送数据时，若遇到后端upstream挂掉或者不通，会收到该错误 “(110: Connection timed out) while connecting to upstream” nginx连接后面的upstream时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(110: Connection timed out) while reading response header from upstream” nginx读取来自upstream的响应头时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(104: Connection reset by peer) while connecting to upstream” upstream发送了RST，将连接重置 “upstream sent invalid header while reading response header from upstream” upstream发送的响应头无效 “upstream sent no valid HTTP/1.0 header while reading response header from upstream” upstream发送的响应头无效 “client intended to send too large body” 用于设置允许接受的客户端请求内容的最大值，默认值是1M，client发送的body超过了设置值 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:1:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Nginx日志没有记录请求，客户端报错EOF 或 Connection reset by peer ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Nginx日志状态码为502 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"负载均衡实例连接数超限(Connection reset by peer) 报错: recv() failed (104: Connection reset by peer) while reading response header from upstream 线上Nginx运行了n年后，业务发现有502错误的错误，通过排查发现这个502已经持续了很久很久，通过error日志发现报错是recv() failed (104: Connection reset by peer) while reading response header from upstream 业务架构 负载均衡: 均使用阿里云的CLB实例 日志排查 排查日志发现, 发现Istio Gateway没有收到Nginx的请求, 所以判断问题出在 Nginx到Istio Gateway 抓包 通过抓包发现，TCP连接建立之后，Nginx发送的HTTP请求并没有发送到Istio 502问题排查-wireshark-rest 解决 通过查看阿里云的SLB监控，发现内网Istio负载均衡有丢包的情况，于是对CLB实例进行了升配，修复了这个问题 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:1","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Connection timed out upstream timed out (110: Connection timed out) while connecting to upstream connect() failed (110: Connection timed out) while connecting to upstream 对CLB实例升配后，虽然502少了特别多，但是还是有502的请求，报错是upstream timed out (110: Connection timed out) while connecting to upstream ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:2","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"解决方案 增加keepalive, 具体可以参考文档: https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive 示例 upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } 后续 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:3","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"为何SLB并发支持的这么小 https://www.alibabacloud.com/help/zh/slb/classic-load-balancer/user-guide/faq-about-clb#section-3kw-gvt-8go 场景描述：在使用少量长连接的业务场景下，转发分组中的系统服务器可能不会全部被分配到长连接，可能会出现CLB实例达不到QPS峰值的现象。 原理（原因）： 因为负载均衡系统通过集群部署的方式为负载均衡实例提供服务，所有外部的访问请求都将平均分散到这些负载均衡系统服务器上进行转发。所以，CLB实例的QPS峰值将被平均设定在多台系统服务器上。 单个系统服务器的QPS上限计算方法为：单个系统服务器QPS峰值=实例总QPS/（N-1）。N为转发分组中系统服务器的个数。例如您在控制台上购买了简约型I（slb.s1.small）规格的CLB实例，对应的QPS为1000，当多客户端同时使用时，总QPS可以达到1000 QPS。若系统服务器个数为8，那么单个系统服务器的最大QPS为1000/(8-1)=142 QPS。 推荐方案： 使用单客户端短连接进行压测。 根据实际业务情况减少连接复用。 升配CLB实例规格。具体操作，请参见按量付费（按规格计费）升降配。 使用ALB实例，此种方案下负载均衡实例具有足够的弹性。 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:4:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"参考文档 Nginx: Connection reset by peer 错误定位 nginx(二十九)error.log记录报错信息分析 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:5:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation","windows"],"content":"powershell设置自动补全 notepad.exe $PROFILE hugo.exe completion powershell | Out-String | Invoke-Expression ","date":"2023-11-30","objectID":"/powershell-completion/:0:0","tags":["windows"],"title":"Powershell Completion","uri":"/powershell-completion/"},{"categories":["documentation","hugo"],"content":"hugo page first page ","date":"2023-11-28","objectID":"/first/:0:0","tags":["hugo"],"title":"First","uri":"/first/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":" 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 扫码关注微信公众号 ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]