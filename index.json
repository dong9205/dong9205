[{"categories":["documentation","kubernetes"],"content":"kubeadm高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:0:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"基础环境初始化 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装Docker,Containerd 配置先决条件 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 通过运行以下指令确认 br_netfilter 和 overlay 模块被加载： lsmod | grep br_netfilter lsmod | grep overlay 通过运行以下指令确认 net.bridge.bridge-nf-call-iptables、net.bridge.bridge-nf-call-ip6tables 和 net.ipv4.ip_forward 系统变量在你的 sysctl 配置中被设置为 1： sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 安装docker，containerd sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin rm -f /etc/containerd/config.toml systemctl start docker 修改containerd配置 报错: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service 需要将SystemdCgroup修改为true: https://github.com/kubernetes/kubeadm/issues/1047 containerd config default \u003e /etc/containerd/config.toml sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装kubeadm 关闭swap 如果 kubelet 未被正确配置使用交换分区，则你必须禁用交换分区。 例如，sudo swapoff -a 将暂时禁用交换分区。要使此更改在重启后保持不变，请确保在如 /etc/fstab、systemd.swap 等配置文件中禁用交换分区，具体取决于你的系统如何配置。 安装 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config sudo yum install -y kubelet-1.27.6 kubeadm-1.27.6 kubectl-1.27.6 --disableexcludes=kubernetes sudo systemctl enable --now kubelet ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"使用内部etcd构建高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"kube-apiserver负载均衡器 使用haproxy+keepalived的方式做apiserver高可用 机器 IP 安装步骤 master1 192.168.25.3 kubeadm安装 master2 192.168.25.5 kubeadm安装 node1 192.168.25.4 kubeadm安装 VIP 192.168.25.6 kubeadm安装 配置haproxy 配置kubelet的静态pod yaml文件 cat /etc/kubernetes/manifests/haproxy.yaml apiVersion: v1 kind: Pod metadata: name: haproxy namespace: kube-system spec: containers: #- image: haproxy:2.1.4 - image: haproxy:2.8.4 name: haproxy livenessProbe: failureThreshold: 8 httpGet: host: localhost path: /healthz port: 30033 scheme: HTTPS volumeMounts: - mountPath: /usr/local/etc/haproxy/haproxy.cfg name: haproxyconf readOnly: true hostNetwork: true volumes: - hostPath: path: /etc/haproxy/haproxy.cfg type: FileOrCreate name: haproxyconf 配置haproxy配置 cat /etc/haproxy/haproxy.cfg # /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log stdout local0 log stdout local1 notice daemon maxconn 75000 #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 1 timeout http-request 10s timeout queue 20s timeout connect 5s timeout client 20s timeout server 20s timeout http-keep-alive 10s timeout check 10s #--------------------------------------------------------------------- # apiserver frontend which proxys to the control plane nodes #--------------------------------------------------------------------- frontend apiserver bind *:30033 mode tcp option tcplog default_backend apiserver #--------------------------------------------------------------------- # round robin balancing for apiserver #--------------------------------------------------------------------- backend apiserver option httpchk GET /healthz http-check expect status 200 mode tcp option ssl-hello-chk balance roundrobin server master1 192.168.25.3:6443 check server master2 192.168.25.5:6443 check 配置keepalived 配置kubelet管理静态POD文件 cat /etc/kubernetes/manifests/keepalived.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: keepalived namespace: kube-system spec: containers: - image: osixia/keepalived:stable name: keepalived resources: {} args: # Fix docker mounted file problems: https://github.com/osixia/docker-keepalived#fix-docker-mounted-file-problems - --copy-service securityContext: capabilities: add: - NET_ADMIN - NET_BROADCAST - NET_RAW volumeMounts: - mountPath: /container/service/keepalived/assets/keepalived.conf name: config - mountPath: /container/service/keepalived/assets/check_apiserver.sh name: check hostNetwork: true volumes: - hostPath: path: /etc/keepalived/keepalived.conf name: config - hostPath: path: /etc/keepalived/check_apiserver.sh name: check 配置keepalived cat /etc/keepalived/keepalived.conf ! /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_apiserver { script \"/container/service/keepalived/assets/check_apiserver.sh\" interval 3 weight -20 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 authentication { auth_type PASS auth_pass private } virtual_ipaddress { 192.168.25.6/26 dev eth0 label eth0:1 } unicast_src_ip 192.168.25.3 unicast_peer { 192.168.25.5 } track_script { check_apiserver } } 配置检测脚本 cat /etc/keepalived/check_apiserver.sh #!/bin/sh errorExit() { echo \"*** $*\" 1\u003e\u00262 exit 1 } curl --silent --max-time 2 --insecure https://localhost:30033/ -o /dev/null || errorExit \"Error GET https://localhost:30033/\" if ip addr | grep -q 192.168.25.6; then curl --silent --max-time 2 --insecure https://192.168.25.6:30033/ -o /dev/null || errorExit \"Error GET https://192.168.25.6:30033/","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"其他问题 kubectl使用watch操作20秒就会断开 原因是因为haproxy设置的20超时，但是kubectl默认的 idle timeout是30秒，将kubectl的idel timeout配置为15秒即可. issue: https://github.com/kubernetes/kubeadm/issues/2227 export HTTP2_READ_IDLE_TIMEOUT_SECONDS=15 v1.27移除–pod-eviction-timeout后怎么修改驱逐POD时长 https://kubernetes.io/zh-cn/blog/2023/03/17/upcoming-changes-in-kubernetes-v1-27/#%E7%A7%BB%E9%99%A4-pod-eviction-timeout-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0 使用污点方式: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions tolerations: - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 120 下线节点 kubectl cordon node1 kubectl drain --ignore-daemonsets node1 # --ignore-daemonsets 选项为忽略DaemonSet类型的pod ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"参考文档 docker安装官方文档: https://docs.docker.com/engine/install/centos/ 容器运行时: https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ 安装 kubeadm: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 高可用拓扑选项: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/ 利用 kubeadm 创建高可用集群: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/ 阿里云使用keepalived: https://developer.aliyun.com/article/438705 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:3:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation"],"content":"一、istio多集群模型介绍 Istio 多集群网格有多种模型，在网络拓扑上分为扁平网络和非扁平网络，在控制面上分为单一控制平面和多控制平面。 Istio 多集群网格有多种模型，在网络拓扑上分为扁平网络和非扁平网络，在控制面上分为单一控制平面和多控制平面。 扁平网络：所有集群都在同一个网络中，可以直接访问到其他集群的服务，不需要通过网关。 优点： 跨集群访问不经过东西向网关，延迟低 缺点：组网较为复杂，Service、Pod 网段不能重叠，借助 VPN 等技术将所有集群的 Pod 网络打通，需要考虑网络安全问题 非扁平网络：集群之间的网络是隔离的，需要通过网关访问其他集群的服务。 优点：不同集群的网络是互相隔离的，安全性更高，不需要打通不同集群的容器网络，不用提前规划集群的网段 缺点：跨集群访问依赖东西向网关，延迟高。东西向网关工作模式是 TLS AUTO_PASSTHROUGH，不支持 HTTP 路由策略。 单控制面：所有集群共用一个控制平面，所有集群的配置都在同一个控制平面中。 优点：所有集群的配置都在同一个控制平面中，集群之间的配置可以共享，部署运维更简单 缺点：控制平面的性能和可用性会受到影响，不适合大规模集群 多控制面：每个集群都有一个独立的控制平面，集群之间的配置不共享。 优点：控制平面的性能和可用性不会受到影响，适合大规模集群 缺点：集群之间的配置不共享，部署运维较为复杂 总体来说 Istio 目前支持 4 种多集群模型：扁平网络单控制面(主从架构)、扁平网络多控制面(多主架构)、非扁平网络单控制面(跨网络主从架构)、非扁平网络多控制面(跨网络多主架构)。其中扁平网络单控制面是最简单的模型，非扁平网络多控制面是最复杂的模型。 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:1:0","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"二、准备工作 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:0","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"创建两个集群 cluster01 cat \u003c\u003cEOF \u003e cluster01-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster01 nodes: - role: control-plane networking: podSubnet: \"10.10.0.0/16\" serviceSubnet: \"10.11.0.0/16\" EOF kind create cluster --config cluster01-kind.yaml # kind默认在kubeconfig中生成的地址是https://127.0.0.1:xxxxx，需要把地址改为容器的IP地址，否则两个集群无法访问 kubectl config set-cluster kind-cluster01 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane):6443 kubectl --context kind-cluster01 apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml # 等待pod ready sleep 10 \u0026\u0026 kubectl --context kind-cluster01 -n metallb-system wait --for=condition=Ready pods -l app=metallb --timeout=600s cat \u003c\u003cEOF | kubectl --context kind-cluster01 create -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: custom-172.168.11.0-255 namespace: metallb-system spec: addresses: - 172.18.11.0-172.18.11.255 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: empty namespace: metallb-system EOF cluster02 cat \u003c\u003cEOF \u003e cluster02-kind.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: cluster02 nodes: - role: control-plane networking: podSubnet: \"10.20.0.0/16\" serviceSubnet: \"10.21.0.0/16\" EOF kind create cluster --config cluster02-kind.yaml kubectl config set-cluster kind-cluster02 --server=https://$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane):6443 kubectl --context kind-cluster02 apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml sleep 10 \u0026\u0026 kubectl --context kind-cluster02 -n metallb-system wait --for=condition=Ready pods -l app=metallb --timeout=600s cat \u003c\u003cEOF | kubectl create --context kind-cluster02 -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: custom-172.168.12.0-255 namespace: metallb-system spec: addresses: - 172.18.12.0-172.18.12.255 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: empty namespace: metallb-system EOF ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:1","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"配置信任关系 默认情况下，Istio CA 会生成一个自签名的根证书和密钥，并使用它们来签署工作负载证书。 为了保护根 CA 密钥，您应该使用在安全机器上离线运行的根 CA，并使用根 CA 向运行在每个集群上的 Istio CA 签发中间证书。Istio CA 可以使用管理员指定的证书和密钥来签署工作负载证书， 并将管理员指定的根证书作为信任根分配给工作负载。 下图展示了在包含两个集群的网格中推荐的 CA 层次结构。 CA层次结构 Makefile.k8s.mk：基于 k8s 集群中的 root-ca 创建证书。默认 kubeconfig 中的当前上下文用于访问集群。 Makefile.selfsigned.mk：基于生成的自签名根创建证书。 下表描述了两个 Makefile 支持的目标 Make Target Makefile Description root-ca Makefile.selfsigned.mk 生成自签名根 CA 密钥和证书. fetch-root-ca Makefile.k8s.mk 使用默认 kubeconfig 中的当前上下文从 Kubernetes 集群获取 Istio CA. $NAME-cacerts Both 为具有 $NAME 的集群或虚拟机（例如 us-east、cluster01 等）生成由根 CA 签名的中间证书。它们存储在 $NAME 目录下。为了区分集群，我们在证书主题字段中包含位置 (L) 名称以及集群名称。 $NAMESPACE-certs Both 使用根证书为使用 serviceAccount $SERVICE_ACCOUNT 连接到命名空间 $NAMESPACE 的虚拟机生成中间证书和签名证书，并将它们存储在 $NAMESPACE 目录下。 clean Both 删除任何生成的根证书、密钥和中间文件。. 创建一个目录来存放证书和密钥 如果您计划仅部署一个主集群（即采用本地——远程部署的方式），您将只有一个 CA （即使用 cluster01 上的 istiod ）为两个集群颁发证书。 在这种情况下，您可以跳过以下 CA 证书生成步骤， 并且只需使用默认自签名的 CA 进行安装。 mkdir -p certs pushd certs 生成根CA证书 make -f ../tools/certs/Makefile.selfsigned.mk root-ca 将会生成以下文件： root-cert.pem：生成的根证书 root-key.pem：生成的根密钥 root-ca.conf：生成根证书的 openssl 配置 root-cert.csr：为根证书生成的 CSR 为cluster生成证书 make -f ../tools/certs/Makefile.selfsigned.mk cluster01-cacerts make -f ../tools/certs/Makefile.selfsigned.mk cluster02-cacerts 运行以上命令，将会在名为 cluster01、cluster02 的目录下生成以下文件： ca-cert.pem：生成的中间证书 ca-key.pem：生成的中间密钥 cert-chain.pem：istiod 使用的生成的证书链 root-cert.pem：根证书 创建cacerts Secret kubectl --context kind-cluster01 create namespace istio-system kubectl --context kind-cluster01 create secret generic cacerts -n istio-system \\ --from-file=cluster01/ca-cert.pem \\ --from-file=cluster01/ca-key.pem \\ --from-file=cluster01/root-cert.pem \\ --from-file=cluster01/cert-chain.pem kubectl --context kind-cluster02 create namespace istio-system kubectl --context kind-cluster02 create secret generic cacerts -n istio-system \\ --from-file=cluster02/ca-cert.pem \\ --from-file=cluster02/ca-key.pem \\ --from-file=cluster02/root-cert.pem \\ --from-file=cluster02/cert-chain.pem ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:2:2","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"三、istio多集群部署 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:0","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"扁平网络单控制面(主从架构) https://istio.io/latest/zh/docs/setup/install/multicluster/primary-remote/ 该模型下只需要将 Istio 控制面组件部署在主集群中，然后可以通过这个控制面来管理所有集群的 Service 和 Endpoint，其他的 Istio 相关的 API 比如 VirtualService、DestinationRule 等也只需要在主集群中配置即可，其他集群不需要部署 Istio 控制面组件。 控制平面的 Istiod 核心组件负责连接所有集群的 kube-apiserver，获取每个集群的 Service、Endpoint、Pod 等信息，所有集群的 Sidecar 均连接到这个中心控制面，由这个中心控制面负责所有的 Envoy Sidecar 的配置生成和分发。 主从架构的安装 多集群扁平网络模型和单一集群的服务网格在访问方式上几乎没什么区别，但是需要注意不同集群的 Service IP 和 Pod 的 IP 不能重叠，否则会导致集群之间的服务发现出现问题，这也是扁平网络模型的一个缺点，需要提前规划好集群的网段。 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --context=\"kind-cluster01\" -f cluster01.yaml -y 需要注意的是，当 values.pilot.env.EXTERNAL_ISTIOD 被设置为 true 时， 安装在 cluster01 上的控制平面也可以作为其他从集群的外部控制平面。 当这个功能被启用时，istiod 将试图获得领导权锁，并因此管理将附加到它的并且带有 适当注解的从集群 （本例中为 cluster02）。 在 cluster01 安装东西向网关 在 cluster01 中安装东西向流量专用网关，默认情况下，此网关将被公开到互联网上。 生产环境可能需要增加额外的准入限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云供应商，了解可用的选项。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster01 --network network1 | istioctl --context=kind-cluster01 install -y -f - 如果控制面已经安装了一个修订版，可在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关获取外部 IP 地址： kubectl --context=kind-cluster01 get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.96.164.163 172.18.11.1 15021:30169/TCP,15443:32267/TCP,15012:30328/TCP,15017:30664/TCP 2m7s 在 cluster01 中开放控制平面 在安装 cluster02 之前，我们需要开放 cluster01 的控制平面， 以便 cluster02 中的服务能访问到服务发现： kubectl apply --context=kind-cluster01 -f /root/istio-1.18.2/samples/multicluster/expose-istiod.yaml 设置集群 cluster02 的控制平面 我们需要通过为 istio-system 命名空间添加注解来识别应管理集群 cluster02 的外部控制平面： kubectl --context kind-cluster02 create namespace istio-system kubectl --context=\"kind-cluster02\" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster01 将 cluster02 设为从集群 保存 cluster01 东西向网关的地址。 export DISCOVERY_ADDRESS=$(kubectl --context=\"kind-cluster01\" -n istio-system get svc istio-eastwestgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 现在，为 cluster02 创建一个从集群配置： cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERV","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:1","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"扁平网络多控制面(多主架构) https://istio.io/latest/zh/docs/setup/install/multicluster/multi-primary/ 同一网络的多主集群 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置文件应用到 cluster01： istioctl install --context=\"kind-cluster01\" -f cluster01.yaml -y 将 cluster02 设为主集群 cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster02 network: network1 logAsJson: true EOF 将配置文件应用到 cluster02： istioctl install --context=\"kind-cluster02\" -f cluster02.yaml -y 开启端点发现 在 cluster02 中安装从集群的 secret，该 secret 提供 cluster01 的 API 服务器的访问权限。 istioctl create-remote-secret \\ --context=\"kind-cluster01\" \\ --name=cluster01 | \\ kubectl apply -f - --context=\"kind-cluster02\" 在 cluster01 中安装从集群的 secret，该 secret 提供 cluster02 的 API 服务器的访问权限。 istioctl create-remote-secret \\ --context=\"kind-cluster02\" \\ --name=cluster02 | \\ kubectl apply -f - --context=\"kind-cluster01\" 两个集群网络打通 docker container exec cluster01-control-plane ip route add 10.20.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane) dev eth0 docker container exec cluster01-control-plane ip route add 10.21.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster02-control-plane) dev eth0 docker container exec cluster02-control-plane ip route add 10.10.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane) dev eth0 docker container exec cluster02-control-plane ip route add 10.11.0.0/16 via $(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' cluster01-control-plane) dev eth0 后续步骤 现在，您可以验证此次安装 ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:2","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"非扁平网络单控制面(跨网络主从架构) https://istio.io/latest/zh/docs/setup/install/multicluster/primary-remote_multi-network 跨网络的主从集群 为 cluster01 设置默认网络 kubectl --context=\"kind-cluster01\" label namespace istio-system topology.istio.io/network=network1 将 cluster01 设为主集群 为 cluster01 创建 Istio 配置： cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置应用到 cluster01： istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --context=\"kind-cluster01\" -f cluster01.yaml -y 请注意，values.pilot.env.EXTERNAL_ISTIOD 设置为 true。 这将启用安装在 cluster01 上的控制平面，使其也用作其他从集群的外部控制平面。 启用此特性后，istiod 将尝试获取领导选举锁， 并因此管理适当注解的且将接入的从集群（此处为 cluster02）。 在 cluster01 安装东西向网关 在 cluster01 安装专用的东西向流量网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要额外的访问限制（即通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh \\ --mesh mesh1 --cluster cluster01 --network network1 | \\ istioctl --context=\"kind-cluster01\" install -y -f - 如果控制平面已随着版本修正一起安装，请在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关获取外部 IP 地址： $ kubectl --context=kind-cluster01 get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.11.43.67 172.18.11.1 15021:31333/TCP,15443:32098/TCP,15012:31996/TCP,15017:30454/TCP 40s 开放 cluster01 控制平面 安装 cluster02 之前，我们需要先开放 cluster01 的控制平面， 以便 cluster02 中的服务能访问服务发现。 kubectl apply --context=\"kind-cluster01\" -n istio-system -f /root/istio-1.18.2/samples/multicluster/expose-istiod.yaml 为 cluster02 设置控制平面集群 命名空间 istio-system 创建之后，我们需要设置集群的网络： 我们需要通过为 istio-system 命名空间添加注解来识别应管理 cluster02 的外部控制平面集群： kubectl --context=\"kind-cluster02\" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster01 将 topology.istio.io/controlPlaneClusters 命名空间注解设置为 cluster01 将指示运行在 cluster01 上的相同命名空间（本例中为 istio-system）中的 istiod 管理作为从集群接入的 cluster02。 为 cluster02 设置默认网络 通过向 istio-system 命名空间添加标签来设置 cluster02 的网络： kubectl --context=\"kind-cluster02\" label namespace istio-system topology.istio.io/network=network2 将 cluster02 设为从集群 保存 cluster01 东西向网关的地址。 export DISCOVERY_ADDRESS=$(kubectl \\ --context=\"kind-cluster01\" \\ -n istio-system get svc istio-eastwestgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 现在，为 cluster02 创建一个从集群配置： cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUE","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:3","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"非扁平网络多控制面(跨网络多主架构) 跨网络的多主集群 为 cluster01 设置缺省网络 kubectl --context=\"kind-cluster01\" label namespace istio-system topology.istio.io/network=network1 将 cluster01 设为主集群 cat \u003c\u003cEOF \u003e cluster01.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster01 network: network1 logAsJson: true EOF 将配置文件应用到 cluster01： istioctl install --context=\"kind-cluster01\" -f cluster01.yaml -y 在 cluster01 安装东西向网关 在 cluster01 安装专用的 东西向网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要添加额外的访问限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18.2/samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster01 --network network1 | istioctl --context=kind-cluster01 install -y -f - 如果控制面已经安装了一个修订版，可以在 gen-eastwest-gateway.sh 命令中添加 –revision rev 标志。 等待东西向网关被分配外部 IP 地址： kubectl --context=\"kind-cluster01\" get svc istio-eastwestgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-eastwestgateway LoadBalancer 10.96.135.178 172.18.11.1 15021:31259/TCP,15443:30115/TCP,15012:30290/TCP,15017:30995/TCP 61s 开放 cluster01 中的服务 因为集群位于不同的网络中，所以我们需要在两个集群东西向网关上开放所有服务（*.local）。 虽然此网关在互联网上是公开的，但它背后的服务只能被拥有可信 mTLS 证书、工作负载 ID 的服务访问， 就像它们处于同一网络一样。 kubectl --context=\"kind-cluster01\" apply -n istio-system -f /root/istio-1.18.2/samples/multicluster/expose-services.yaml 为 cluster02 设置缺省网络 kubectl --context=\"kind-cluster02\" label namespace istio-system topology.istio.io/network=network2 将 cluster02 设为主集群 cat \u003c\u003cEOF \u003e cluster02.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout accessLogEncoding: JSON accessLogFormat: '{\"authority\":\"%REQ(:AUTHORITY)%\",\"bytes_received\":\"%BYTES_RECEIVED%\",\"bytes_sent\":\"%BYTES_SENT%\",\"downstream_local_address\":\"%DOWNSTREAM_LOCAL_ADDRESS%\",\"downstream_remote_address\":\"%DOWNSTREAM_REMOTE_ADDRESS%\",\"duration\":\"%DURATION%\",\"istio_policy_status\":\"%DYNAMIC_METADATA(istio.mixer:status)%\",\"method\":\"%REQ(:METHOD)%\",\"path\":\"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\"protocol\":\"%PROTOCOL%\",\"request_id\":\"%REQ(X-REQUEST-ID)%\",\"requested_server_name\":\"%REQUESTED_SERVER_NAME%\",\"response_code\":\"%RESPONSE_CODE%\",\"response_flags\":\"%RESPONSE_FLAGS%\",\"route_name\":\"%ROUTE_NAME%\",\"start_time\":\"%START_TIME%\",\"trace_id\":\"%REQ(X-B3-TRACEID)%\",\"upstream_cluster\":\"%UPSTREAM_CLUSTER%\",\"upstream_host\":\"%UPSTREAM_HOST%\",\"upstream_local_address\":\"%UPSTREAM_LOCAL_ADDRESS%\",\"upstream_service_time\":\"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\"upstream_transport_failure_reason\":\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\"user_agent\":\"%REQ(USER-AGENT)%\",\"x_forwarded_for\":\"%REQ(X-FORWARDED-FOR)%\"}' values: global: meshID: mesh1 multiCluster: clusterName: cluster02 network: network2 logAsJson: true EOF 将配置文件应用到 cluster02： istioctl install --context=\"kind-cluster02\" -f cluster02.yaml -y 在 cluster02 安装东西向网关 在 cluster02 安装专用的 东西向网关。 默认情况下，此网关将被公开到互联网上。 生产系统可能需要添加额外的访问限制（即：通过防火墙规则）来防止外部攻击。 咨询您的云服务商，了解可用的选择。 /root/istio-1.18","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:4","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"验证安装结果 我们将在 cluster01 安装 V1 版的 HelloWorld 应用程序， 在 cluster02 安装 V2 版的 HelloWorld 应用程序。 当处理一个请求时，HelloWorld 会在响应消息中包含它自身的版本号。 我们也会在两个集群中均部署 Sleep 容器。 这些 Pod 将被用作客户端（source），发送请求给 HelloWorld。 最后，通过收集这些流量数据，我们将能观测并识别出是那个集群处理了请求。 部署服务 HelloWorld 为了支持从任意集群中调用 HelloWorld 服务，每个集群的 DNS 解析必须可用 （详细信息，参见部署模型 7）。 我们通过在网格的每一个集群中部署 HelloWorld 服务，来解决这个问题， 首先，在每个集群中创建命名空间 sample： kubectl create --context=\"kind-cluster01\" namespace sample kubectl create --context=\"kind-cluster02\" namespace sample 为命名空间 sample 开启 sidecar 自动注入： kubectl label --context=\"kind-cluster01\" namespace sample istio-injection=enabled kubectl label --context=\"kind-cluster02\" namespace sample istio-injection=enabled 在每个集群中创建 HelloWorld 服务： kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l service=helloworld -n sample kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l service=helloworld -n sample 部署 V1 版的 HelloWorld kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l version=v1 -n sample 部署 V2 版的 HelloWorld 把应用 helloworld-v2 部署到 cluster02： kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/helloworld/helloworld.yaml \\ -l version=v2 -n sample 部署 Sleep 把应用 Sleep 部署到每个集群： kubectl apply --context=\"kind-cluster01\" \\ -f /root/istio-1.18.2/samples/sleep/sleep.yaml -n sample kubectl apply --context=\"kind-cluster02\" \\ -f /root/istio-1.18.2/samples/sleep/sleep.yaml -n sample 等待POD启动成本 kubectl --context kind-cluster01 -n sample wait --for=condition=Ready pods -l topology.istio.io/network=network1 --timeout=600s kubectl --context kind-cluster02 -n sample wait --for=condition=Ready pods -l topology.istio.io/network=network2 --timeout=600s 验证跨集群流量 要验证跨集群负载均衡是否按预期工作，需要用 Sleep pod 重复调用服务 HelloWorld。 为了确认负载均衡按预期工作，需要从所有集群调用服务 HelloWorld。 从 cluster01 中的 Sleep pod 发送请求给服务 HelloWorld, 重复几次这个请求，验证 HelloWorld 的版本在 v1 和 v2 之间切换： for i in $(seq 10);do kubectl exec --context=\"kind-cluster01\" -n sample -c sleep \"$(kubectl get pod --context=\"kind-cluster01\" -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}')\" -- curl -s helloworld.sample:5000/hello;done Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz 现在，用 cluster02 中的 Sleep pod 重复此过程，重复几次这个请求，验证 HelloWorld 的版本在 v1 和 v2 之间切换： for i in $(seq 10);do kubectl exec --context=\"kind-cluster02\" -n sample -c sleep \"$(kubectl get pod --context=\"kind-cluster02\" -n sample -l app=sleep -o jsonpath='{.items[0].metadata.name}')\" -- curl -s helloworld.sample:5000/hello;done Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx Hello version: v2, instance: helloworld-v2-f976ddc5b-c6ptz Hello version: v1, instance: helloworld-v1-94b6f7986-twskx ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:3:5","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":"参考文档 官方文档: 多集群安装 k8s技术圈：Istio多集群实践 metallb: Installation By Manifest ","date":"2024-01-15","objectID":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/:4:0","tags":[""],"title":"istio多集群安装","uri":"/istio%E5%A4%9A%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 本文摘自 istio 学习笔记 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:0:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"背景 Istio 使用 Envoy 作为数据面转发 HTTP 请求，而 Envoy 默认要求使用 HTTP/1.1 或 HTTP/2，当客户端使用 HTTP/1.0 时就会返回 426 Upgrade Required。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:1:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"常见的 nginx 场景 如果用 nginx 进行 proxy_pass 反向代理，默认会用 HTTP/1.0，你可以显示指定 proxy_http_version 为 1.1: upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:2:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"压测场景 ab 压测时会发送 HTTP/1.0 的请求，Envoy 固定返回 426 Upgrade Required，根本不会进行转发，所以压测的结果也不会准确。可以换成其它压测工具，如 wrk 。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:3:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"让 istio 支持 HTTP/1.0 有些 SDK 或框架可能会使用 HTTP/1.0 协议，比如使用 HTTP/1.0 去资源中心 / 配置中心 拉取配置信息，在不想改动代码的情况下让服务跑在 istio 上，也可以修改 istiod 配置，加上 PILOT_HTTP10: 1 的环境变量来启用 HTTP/1.0。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:4:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"参考资料 Envoy won’t connect to my HTTP/1.0 service ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:5:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 www.jianshu.com 本文结合域名请求慢的问题，从虚拟网络定位到域名解析，根据 coredns 添加域名后缀的机制，定位 coredns 解析慢的根因。 问题现象 ======= 背景：项目是微服务 + flink，其中 flink 采用 k8s session standalone 的部署模式。 问题：微服务通过 flink restful api 启动作业的平均时长超过 40s，导致客户端超时和作业失联。 #为了排除 flink 内部启动作业耗时因素，使用 /jobs/overview 测试微服务到 flink 的网络耗时 $time curl http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview {\"jobs\":[{\"jid\":\"0bf232afb7febbb8b070221833ddb99c\",\"name\":\"TopSpeedWindowing\",\"state\":\"RUNNING\",\"start-time\":1631687894914,\"end-time\":-1,\"duration\":701488855,\"last-modification\":1631687895895,\"tasks\":{\"total\":3,\"created\":0,\"scheduled\":0,\"deploying\":0,\"running\":3,\"finished\":0,\"canceling\":0,\"canceled\":0,\"failed\":0,\"reconciling\":0}}]} real 0m10.528s user 0m0.004s sys 0m0.005s 根因定位 ======= 定位思路：理解 Service 【K8s 精选】Kubernetes Service 介绍。在使用 service name 即域名的场景下，Client Pod3 首先拿着域名去 coredns 解析成 ClusterIP，接着去请求 ClusterIP，最后通过 Kube-Proxy 把请求转发到目标后端 Pod。 Kubernetes 服务发现架构. JPG ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:0:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.1 虚拟化网络分析 2.1.1 curl 命令详解 参考 curl 命令详解，例如 curl --header \"Content-Type: application/json\" -X POST --data '{\"text\":\"germany\"}' https://labs.tib.eu/falcon/api?mode=short 2.1.2 curl 获取 http 各阶段的响应时间 参考通过 curl 得到 http 各阶段的响应时间 ① time_namelookup：DNS 解析时间 ② time_connect：连接时间，从请求开始到 DNS 解析完毕所用时间。单纯的连接时间 = time_connect - time_namelookup ③ time_appconnect：建立完成时间，例如 SSL/SSH 等建立连接或者完成三次握手的时间。 ④ time_redirect：重定向时间，包括最后一次传输前的几次重定向的 DNS 解析、连接、预传输、传输时间。 ⑤ time_pretransfer： 从开始到准备传输的时间。 ⑥ time_starttransfer：开始传输时间。在 client 发出请求后，服务端返回数据的第一个字节所用的时间。 进入业务容器，编辑完获取数据的格式后，执行 curl 命令。 /dev/null 表示空设备，即丢弃一切写入的数据，但显示写入操作成功。 $vim curl-format.txt time_namelookup: %{time_namelookup}\\n time_connect: %{time_connect}\\n time_appconnect: %{time_appconnect}\\n time_redirect: %{time_redirect}\\n time_pretransfer: %{time_pretransfer}\\n time_starttransfer: %{time_starttransfer}\\n ----------\\n time_total: %{time_total}\\n $kubectl get svc |grep flink flink-jobmanager ClusterIP 10.96.0.123 \u003cnone\u003e 8123/TCP,8124/TCP,8091/TCP 4d22h # 首先使用 ClusterIP 测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://10.96.0.123:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 81865 0 --:--:-- --:--:-- --:--:-- 94285 time_namelookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 # 然后使用 service name 即域名测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 62 0 0:00:10 0:00:10 --:--:-- 164 time_melookup: 10.516 time_connect: 10.517 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 10.517 time_starttransfer: 10.520 ---------- time_total: 10.520 对比 ClusterIP 和 service name 的接口调用时长，由 time_namelookup 可知 DNS 解析时间长。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:1:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2 域名解析分析 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.1 外部分析 - coredns 解析域名 $kubectl logs coredns-66509f5cf2-km1q4 -nkube-system 2021-09-23T01:54:04.590Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:32960-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:54:09.592Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:59978-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:00.609Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:41797-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:02.610Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:48375-\u003e100.79.1.250:53: i/o timeout 由 coredns 后台关键日志 A: read udp xxx-\u003exxx: i/o timeout 可知 IPV4 解析超时，AAAA: read udp xxx-\u003exxx: i/o timeout 可知 IPV6 解析也超时。 IPV4 和 IPV6 耗时对比 # IPV4 请求耗时 $curl -4 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 time_melookup: 0.000 time_connect: 0.000 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.000 time_starttransfer: 0.000 ---------- time_total: 3.510 # IPV6 请求耗时 $curl -6 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 146 0 0:00:04 0:00:04 --:--:-- 146 time_melookup: 4.511 time_connect: 4.512 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 4.512 time_starttransfer: 4.515 ---------- time_total: 4.515 结论：IPV6 解析比 IPV4 多耗时约 20%，说明 IPV6 对域名解析有一定的影响，建议 coredns 关闭 IPV6 解析。然而直接用 IPV4 解析也耗时 3s+，需要进一步对容器内部进行抓包分析。 建议：如果 IPV6 模式没有使用，可以关闭。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:1","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.2 内部分析 - 容器内部解析域名 通过宿主机抓起 Pod 网络数据报 nsenter 可以进入 Pod 容器 net 命名空间，同时提供一个快速进入 Pod 容器 net 命名空间脚本，可以参考在容器环境使用 tcpdump 抓包 # 步骤1 获取容器的 pid $ docker ps |grep flink-jobmanager-7b58565dc8-msgpp 1386ce6244ae 192.168.31.37:5000/flink \"bash -cx /opt/flink/s…\" 3 weeks ago Up 3 weeks k8s_flink-jobmanager-7b58565dc8-msgpp_4b6d15d8-7b54-41fb-bf46-ffa91aa33963_0 $ docker inspect 1386ce6244ae| grep Pid \"Pid\": 63046, \"PidMode\": \"\", \"PidsLimit\": 0, # 步骤2 根据 pid 执行命令 # 53 是 corends 域名解析端口 $ nsenter -t 63046 -n $ tcpdump 'src host 10.244.3.81 and src port 8091' # 步骤3 打开新窗口，进入容器并执行上述的 curl 命令 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" 下载 tcpdump 的抓包数据 xxx.pcap， 利用 wireshark 分析 tcpdump 报文，其结果如下： ① flink-jobmanager.default.svc.cluster.local 域名解析成 ip 的时间约 10s ② 在域名解析的过程中，在 flink-jobmanager.default.svc.cluster.local 的基础上，其后缀依次添加 default.svc.cluster.local、svc.cluster.local、cluster.local、openstacklocal 查看业务容器中的配置 /etc/resolv.conf，发现上述的后缀恰好是 search 内容。 $ cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen 3 容器 /etc/resolv.conf 配置分析 容器 /etc/resolv.conf 配置如下： nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:2","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"① nameserver resolv.conf 文件的第一行是 nameserver，内容是 coredns 的 ClusterIP $kubectl get svc -nkube-system |grep dns kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP 167d ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:3","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"② search resolv.conf 文件的第一行是 search。当域名解析的时候，将域名依次添加后缀，例如： flink-jobmanager.default.svc.cluster.local.default.svc.cluster.local flink-jobmanager.default.svc.cluster.local.svc.cluster.local flink-jobmanager.default.svc.cluster.local.cluster.local flink-jobmanager.default.svc.cluster.local.openstacklocal ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:4","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"③ options resolv.conf 文件的第一行是 options 其它项，常见配置是 ndots。ndots: 5 表示如果域名包含的 “.” 少于 5 个，则先添加 search 后缀，再使用绝对域名；如果域名包含的 “.” 大于等于 5 个，则先使用绝对域名，再添加 search 后缀。 #样例1 域名a.b.c.d.e，则域名的顺序如下 a.b.c.d.e.default.svc.cluster.local a.b.c.d.e.svc.cluster.local a.b.c.d.e.cluster.local a.b.c.d.e.openstacklocal a.b.c.d.e #样例2 域名a.b.c.d.e.f，则域名的顺序如下 a.b.c.d.e.f a.b.c.d.e.f.default.svc.cluster.local a.b.c.d.e.f.svc.cluster.local a.b.c.d.e.f.cluster.local a.b.c.d.e.f.openstacklocal 结合 flink-jobmanager.default.svc.cluster.local 域名解析慢的问题，由于该域名包含 “.” 等于 4，即少于 5，所以会首先依次添加后缀 default.svc.cluster.local、svc.cluster.local、cluster.local openstacklocal。因此，解决方案有 ①flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager；②ndots: 5 修改为 ndots: 4。 4 解决方案 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:5","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.1 使用简洁域名 将访问的域名从 flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager，效果如下： curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:6","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.2 resolv.conf 配置更改 将 /etc/resolv.conf 的 options 配置 ndots: 5 修改为 ndots: 4，或者修改 deployment 的 yaml 配置，效果如下： # 修改 deployment 的 yaml 配置 # spec.spec.dnsConfig.options[].ndots dnsConfig: options: - name: ndots value: 4 - name: single-request-reopen curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.005 time_connect: 0.006 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:7","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"Nginx 502问题排查 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:0:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"nginx快速定位异常 错误信息 错误说明 “upstream prematurely（过早的） closed connection” 请求uri的时候出现的异常，是由于upstream还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略 “recv() failed (104: Connection reset by peer)” （1）服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉； （2）客户关掉了浏览器，而服务器还在给客户端发送数据； （3）浏览器端按了Stop “(111: Connection refused) while connecting to upstream” 用户在连接时，若遇到后端upstream挂掉或者不通，会收到该错误 “(111: Connection refused) while reading response header from upstream” 用户在连接成功后读取数据时，若遇到后端upstrream挂掉或者不通，会收到该错误 “(111: Connection refused) while sending request to upstream” Nginx和upstream连接成功后发送数据时，若遇到后端upstream挂掉或者不通，会收到该错误 “(110: Connection timed out) while connecting to upstream” nginx连接后面的upstream时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(110: Connection timed out) while reading response header from upstream” nginx读取来自upstream的响应头时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(104: Connection reset by peer) while connecting to upstream” upstream发送了RST，将连接重置 “upstream sent invalid header while reading response header from upstream” upstream发送的响应头无效 “upstream sent no valid HTTP/1.0 header while reading response header from upstream” upstream发送的响应头无效 “client intended to send too large body” 用于设置允许接受的客户端请求内容的最大值，默认值是1M，client发送的body超过了设置值 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:1:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Nginx日志状态码为502 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"负载均衡实例连接数超限(Connection reset by peer) 报错: recv() failed (104: Connection reset by peer) while reading response header from upstream 线上Nginx运行了n年后，业务发现有502错误的错误，通过排查发现这个502已经持续了很久很久，通过error日志发现报错是recv() failed (104: Connection reset by peer) while reading response header from upstream 业务架构 负载均衡: 均使用阿里云的CLB实例 日志排查 排查日志发现, 发现Istio Gateway没有收到Nginx的请求, 所以判断问题出在 Nginx到Istio Gateway 抓包 通过抓包发现，TCP连接建立之后，Nginx发送的HTTP请求并没有发送到Istio 502问题排查-wireshark-rest 解决 通过查看阿里云的SLB监控，发现内网Istio负载均衡有丢包的情况，于是对CLB实例进行了升配，修复了这个问题 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:1","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Connection timed out upstream timed out (110: Connection timed out) while connecting to upstream connect() failed (110: Connection timed out) while connecting to upstream 对CLB实例升配后，虽然502少了特别多，但是还是有502的请求，报错是upstream timed out (110: Connection timed out) while connecting to upstream ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:2","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"解决方案 增加keepalive, 具体可以参考文档: https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive 示例 upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } 后续 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:3","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"为何SLB并发支持的这么小 https://www.alibabacloud.com/help/zh/slb/classic-load-balancer/user-guide/faq-about-clb#section-3kw-gvt-8go 场景描述：在使用少量长连接的业务场景下，转发分组中的系统服务器可能不会全部被分配到长连接，可能会出现CLB实例达不到QPS峰值的现象。 原理（原因）： 因为负载均衡系统通过集群部署的方式为负载均衡实例提供服务，所有外部的访问请求都将平均分散到这些负载均衡系统服务器上进行转发。所以，CLB实例的QPS峰值将被平均设定在多台系统服务器上。 单个系统服务器的QPS上限计算方法为：单个系统服务器QPS峰值=实例总QPS/（N-1）。N为转发分组中系统服务器的个数。例如您在控制台上购买了简约型I（slb.s1.small）规格的CLB实例，对应的QPS为1000，当多客户端同时使用时，总QPS可以达到1000 QPS。若系统服务器个数为8，那么单个系统服务器的最大QPS为1000/(8-1)=142 QPS。 推荐方案： 使用单客户端短连接进行压测。 根据实际业务情况减少连接复用。 升配CLB实例规格。具体操作，请参见按量付费（按规格计费）升降配。 使用ALB实例，此种方案下负载均衡实例具有足够的弹性。 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"参考文档 Nginx: Connection reset by peer 错误定位 nginx(二十九)error.log记录报错信息分析 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:4:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation","windows"],"content":"powershell设置自动补全 notepad.exe $PROFILE hugo.exe completion powershell | Out-String | Invoke-Expression ","date":"2023-11-30","objectID":"/powershell-completion/:0:0","tags":["windows"],"title":"Powershell Completion","uri":"/powershell-completion/"},{"categories":["documentation","hugo"],"content":"hugo page first page ","date":"2023-11-28","objectID":"/first/:0:0","tags":["hugo"],"title":"First","uri":"/first/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"2019-08-02","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 73 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"2019-08-02","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"2019-08-02","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]