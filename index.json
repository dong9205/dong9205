[{"categories":["documentation","kubernetes"],"content":"kubeadm高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:0:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"基础环境初始化 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装Docker,Containerd 配置先决条件 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system 通过运行以下指令确认 br_netfilter 和 overlay 模块被加载： lsmod | grep br_netfilter lsmod | grep overlay 通过运行以下指令确认 net.bridge.bridge-nf-call-iptables、net.bridge.bridge-nf-call-ip6tables 和 net.ipv4.ip_forward 系统变量在你的 sysctl 配置中被设置为 1： sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 安装docker，containerd sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin rm -f /etc/containerd/config.toml systemctl start docker 修改containerd配置 报错: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service 需要将SystemdCgroup修改为true: https://github.com/kubernetes/kubeadm/issues/1047 containerd config default \u003e /etc/containerd/config.toml sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"安装kubeadm 关闭swap 如果 kubelet 未被正确配置使用交换分区，则你必须禁用交换分区。 例如，sudo swapoff -a 将暂时禁用交换分区。要使此更改在重启后保持不变，请确保在如 /etc/fstab、systemd.swap 等配置文件中禁用交换分区，具体取决于你的系统如何配置。 安装 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config sudo yum install -y kubelet-1.27.6 kubeadm-1.27.6 kubectl-1.27.6 --disableexcludes=kubernetes sudo systemctl enable --now kubelet ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:1:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"使用内部etcd构建高可用集群 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"kube-apiserver负载均衡器 使用haproxy+keepalived的方式做apiserver高可用 机器 IP 安装步骤 master1 192.168.25.3 kubeadm安装 master2 192.168.25.5 kubeadm安装 node1 192.168.25.4 kubeadm安装 VIP 192.168.25.6 kubeadm安装 配置haproxy 配置kubelet的静态pod yaml文件 cat /etc/kubernetes/manifests/haproxy.yaml apiVersion: v1 kind: Pod metadata: name: haproxy namespace: kube-system spec: containers: #- image: haproxy:2.1.4 - image: haproxy:2.8.4 name: haproxy livenessProbe: failureThreshold: 8 httpGet: host: localhost path: /healthz port: 30033 scheme: HTTPS volumeMounts: - mountPath: /usr/local/etc/haproxy/haproxy.cfg name: haproxyconf readOnly: true hostNetwork: true volumes: - hostPath: path: /etc/haproxy/haproxy.cfg type: FileOrCreate name: haproxyconf 配置haproxy配置 cat /etc/haproxy/haproxy.cfg # /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log stdout local0 log stdout local1 notice daemon maxconn 75000 #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 1 timeout http-request 10s timeout queue 20s timeout connect 5s timeout client 20s timeout server 20s timeout http-keep-alive 10s timeout check 10s #--------------------------------------------------------------------- # apiserver frontend which proxys to the control plane nodes #--------------------------------------------------------------------- frontend apiserver bind *:30033 mode tcp option tcplog default_backend apiserver #--------------------------------------------------------------------- # round robin balancing for apiserver #--------------------------------------------------------------------- backend apiserver option httpchk GET /healthz http-check expect status 200 mode tcp option ssl-hello-chk balance roundrobin server master1 192.168.25.3:6443 check server master2 192.168.25.5:6443 check 配置keepalived 配置kubelet管理静态POD文件 cat /etc/kubernetes/manifests/keepalived.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: keepalived namespace: kube-system spec: containers: - image: osixia/keepalived:stable name: keepalived resources: {} args: # Fix docker mounted file problems: https://github.com/osixia/docker-keepalived#fix-docker-mounted-file-problems - --copy-service securityContext: capabilities: add: - NET_ADMIN - NET_BROADCAST - NET_RAW volumeMounts: - mountPath: /container/service/keepalived/assets/keepalived.conf name: config - mountPath: /container/service/keepalived/assets/check_apiserver.sh name: check hostNetwork: true volumes: - hostPath: path: /etc/keepalived/keepalived.conf name: config - hostPath: path: /etc/keepalived/check_apiserver.sh name: check 配置keepalived cat /etc/keepalived/keepalived.conf ! /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_apiserver { script \"/container/service/keepalived/assets/check_apiserver.sh\" interval 3 weight -20 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 authentication { auth_type PASS auth_pass private } virtual_ipaddress { 192.168.25.6/26 dev eth0 label eth0:1 } unicast_src_ip 192.168.25.3 unicast_peer { 192.168.25.5 } track_script { check_apiserver } } 配置检测脚本 cat /etc/keepalived/check_apiserver.sh #!/bin/sh errorExit() { echo \"*** $*\" 1\u003e\u00262 exit 1 } curl --silent --max-time 2 --insecure https://localhost:30033/ -o /dev/null || errorExit \"Error GET https://localhost:30033/\" if ip addr | grep -q 192.168.25.6; then curl --silent --max-time 2 --insecure https://192.168.25.6:30033/ -o /dev/null || errorExit \"Error GET https://192.168.25.6:30033/","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:1","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"其他问题 kubectl使用watch操作20秒就会断开 原因是因为haproxy设置的20超时，但是kubectl默认的 idle timeout是30秒，将kubectl的idel timeout配置为15秒即可. issue: https://github.com/kubernetes/kubeadm/issues/2227 export HTTP2_READ_IDLE_TIMEOUT_SECONDS=15 v1.27移除–pod-eviction-timeout后怎么修改驱逐POD时长 https://kubernetes.io/zh-cn/blog/2023/03/17/upcoming-changes-in-kubernetes-v1-27/#%E7%A7%BB%E9%99%A4-pod-eviction-timeout-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0 使用污点方式: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions tolerations: - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 120 下线节点 kubectl cordon node1 kubectl drain --ignore-daemonsets node1 # --ignore-daemonsets 选项为忽略DaemonSet类型的pod ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:2:2","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation","kubernetes"],"content":"参考文档 docker安装官方文档: https://docs.docker.com/engine/install/centos/ 容器运行时: https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ 安装 kubeadm: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 高可用拓扑选项: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/ 利用 kubeadm 创建高可用集群: https://v1-27.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/ 阿里云使用keepalived: https://developer.aliyun.com/article/438705 ","date":"2023-11-30","objectID":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/:3:0","tags":["kubernetes","linux","haproxy","keepalived","高可用"],"title":"Kubeadm高可用集群","uri":"/kubeadm%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 本文摘自 istio 学习笔记 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:0:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"背景 Istio 使用 Envoy 作为数据面转发 HTTP 请求，而 Envoy 默认要求使用 HTTP/1.1 或 HTTP/2，当客户端使用 HTTP/1.0 时就会返回 426 Upgrade Required。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:1:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"常见的 nginx 场景 如果用 nginx 进行 proxy_pass 反向代理，默认会用 HTTP/1.0，你可以显示指定 proxy_http_version 为 1.1: upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:2:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"压测场景 ab 压测时会发送 HTTP/1.0 的请求，Envoy 固定返回 426 Upgrade Required，根本不会进行转发，所以压测的结果也不会准确。可以换成其它压测工具，如 wrk 。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:3:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"让 istio 支持 HTTP/1.0 有些 SDK 或框架可能会使用 HTTP/1.0 协议，比如使用 HTTP/1.0 去资源中心 / 配置中心 拉取配置信息，在不想改动代码的情况下让服务跑在 istio 上，也可以修改 istiod 配置，加上 PILOT_HTTP10: 1 的环境变量来启用 HTTP/1.0。 ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:4:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":"参考资料 Envoy won’t connect to my HTTP/1.0 service ","date":"2023-12-28","objectID":"/426%E7%8A%B6%E6%80%81%E7%A0%81/:5:0","tags":[""],"title":"istio 常见问题: 返回 426 状态码","uri":"/426%E7%8A%B6%E6%80%81%E7%A0%81/"},{"categories":["documentation"],"content":" 本文由 简悦 SimpRead 转码， 原文地址 www.jianshu.com 本文结合域名请求慢的问题，从虚拟网络定位到域名解析，根据 coredns 添加域名后缀的机制，定位 coredns 解析慢的根因。 问题现象 ======= 背景：项目是微服务 + flink，其中 flink 采用 k8s session standalone 的部署模式。 问题：微服务通过 flink restful api 启动作业的平均时长超过 40s，导致客户端超时和作业失联。 #为了排除 flink 内部启动作业耗时因素，使用 /jobs/overview 测试微服务到 flink 的网络耗时 $time curl http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview {\"jobs\":[{\"jid\":\"0bf232afb7febbb8b070221833ddb99c\",\"name\":\"TopSpeedWindowing\",\"state\":\"RUNNING\",\"start-time\":1631687894914,\"end-time\":-1,\"duration\":701488855,\"last-modification\":1631687895895,\"tasks\":{\"total\":3,\"created\":0,\"scheduled\":0,\"deploying\":0,\"running\":3,\"finished\":0,\"canceling\":0,\"canceled\":0,\"failed\":0,\"reconciling\":0}}]} real 0m10.528s user 0m0.004s sys 0m0.005s 根因定位 ======= 定位思路：理解 Service 【K8s 精选】Kubernetes Service 介绍。在使用 service name 即域名的场景下，Client Pod3 首先拿着域名去 coredns 解析成 ClusterIP，接着去请求 ClusterIP，最后通过 Kube-Proxy 把请求转发到目标后端 Pod。 Kubernetes 服务发现架构. JPG ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:0:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.1 虚拟化网络分析 2.1.1 curl 命令详解 参考 curl 命令详解，例如 curl --header \"Content-Type: application/json\" -X POST --data '{\"text\":\"germany\"}' https://labs.tib.eu/falcon/api?mode=short 2.1.2 curl 获取 http 各阶段的响应时间 参考通过 curl 得到 http 各阶段的响应时间 ① time_namelookup：DNS 解析时间 ② time_connect：连接时间，从请求开始到 DNS 解析完毕所用时间。单纯的连接时间 = time_connect - time_namelookup ③ time_appconnect：建立完成时间，例如 SSL/SSH 等建立连接或者完成三次握手的时间。 ④ time_redirect：重定向时间，包括最后一次传输前的几次重定向的 DNS 解析、连接、预传输、传输时间。 ⑤ time_pretransfer： 从开始到准备传输的时间。 ⑥ time_starttransfer：开始传输时间。在 client 发出请求后，服务端返回数据的第一个字节所用的时间。 进入业务容器，编辑完获取数据的格式后，执行 curl 命令。 /dev/null 表示空设备，即丢弃一切写入的数据，但显示写入操作成功。 $vim curl-format.txt time_namelookup: %{time_namelookup}\\n time_connect: %{time_connect}\\n time_appconnect: %{time_appconnect}\\n time_redirect: %{time_redirect}\\n time_pretransfer: %{time_pretransfer}\\n time_starttransfer: %{time_starttransfer}\\n ----------\\n time_total: %{time_total}\\n $kubectl get svc |grep flink flink-jobmanager ClusterIP 10.96.0.123 \u003cnone\u003e 8123/TCP,8124/TCP,8091/TCP 4d22h # 首先使用 ClusterIP 测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://10.96.0.123:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 81865 0 --:--:-- --:--:-- --:--:-- 94285 time_namelookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 # 然后使用 service name 即域名测试接口调用时长 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 62 0 0:00:10 0:00:10 --:--:-- 164 time_melookup: 10.516 time_connect: 10.517 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 10.517 time_starttransfer: 10.520 ---------- time_total: 10.520 对比 ClusterIP 和 service name 的接口调用时长，由 time_namelookup 可知 DNS 解析时间长。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:1:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2 域名解析分析 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:0","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.1 外部分析 - coredns 解析域名 $kubectl logs coredns-66509f5cf2-km1q4 -nkube-system 2021-09-23T01:54:04.590Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:32960-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:54:09.592Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. A: read udp 10.244.0.18:59978-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:00.609Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:41797-\u003e100.79.1.250:53: i/o timeout 2021-09-23T01:56:02.610Z [ERROR] plugin/errors: 2 flink-jobmanager.default.svc.cluster.local.openstacklocal. AAAA: read udp 10.244.2.19:48375-\u003e100.79.1.250:53: i/o timeout 由 coredns 后台关键日志 A: read udp xxx-\u003exxx: i/o timeout 可知 IPV4 解析超时，AAAA: read udp xxx-\u003exxx: i/o timeout 可知 IPV6 解析也超时。 IPV4 和 IPV6 耗时对比 # IPV4 请求耗时 $curl -4 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 time_melookup: 0.000 time_connect: 0.000 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.000 time_starttransfer: 0.000 ---------- time_total: 3.510 # IPV6 请求耗时 $curl -6 -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 146 0 0:00:04 0:00:04 --:--:-- 146 time_melookup: 4.511 time_connect: 4.512 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 4.512 time_starttransfer: 4.515 ---------- time_total: 4.515 结论：IPV6 解析比 IPV4 多耗时约 20%，说明 IPV6 对域名解析有一定的影响，建议 coredns 关闭 IPV6 解析。然而直接用 IPV4 解析也耗时 3s+，需要进一步对容器内部进行抓包分析。 建议：如果 IPV6 模式没有使用，可以关闭。 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:1","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"2.2.2 内部分析 - 容器内部解析域名 通过宿主机抓起 Pod 网络数据报 nsenter 可以进入 Pod 容器 net 命名空间，同时提供一个快速进入 Pod 容器 net 命名空间脚本，可以参考在容器环境使用 tcpdump 抓包 # 步骤1 获取容器的 pid $ docker ps |grep flink-jobmanager-7b58565dc8-msgpp 1386ce6244ae 192.168.31.37:5000/flink \"bash -cx /opt/flink/s…\" 3 weeks ago Up 3 weeks k8s_flink-jobmanager-7b58565dc8-msgpp_4b6d15d8-7b54-41fb-bf46-ffa91aa33963_0 $ docker inspect 1386ce6244ae| grep Pid \"Pid\": 63046, \"PidMode\": \"\", \"PidsLimit\": 0, # 步骤2 根据 pid 执行命令 # 53 是 corends 域名解析端口 $ nsenter -t 63046 -n $ tcpdump 'src host 10.244.3.81 and src port 8091' # 步骤3 打开新窗口，进入容器并执行上述的 curl 命令 $curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" 下载 tcpdump 的抓包数据 xxx.pcap， 利用 wireshark 分析 tcpdump 报文，其结果如下： ① flink-jobmanager.default.svc.cluster.local 域名解析成 ip 的时间约 10s ② 在域名解析的过程中，在 flink-jobmanager.default.svc.cluster.local 的基础上，其后缀依次添加 default.svc.cluster.local、svc.cluster.local、cluster.local、openstacklocal 查看业务容器中的配置 /etc/resolv.conf，发现上述的后缀恰好是 search 内容。 $ cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen 3 容器 /etc/resolv.conf 配置分析 容器 /etc/resolv.conf 配置如下： nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal options ndots:5 single-request-reopen ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:2","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"① nameserver resolv.conf 文件的第一行是 nameserver，内容是 coredns 的 ClusterIP $kubectl get svc -nkube-system |grep dns kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP 167d ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:3","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"② search resolv.conf 文件的第一行是 search。当域名解析的时候，将域名依次添加后缀，例如： flink-jobmanager.default.svc.cluster.local.default.svc.cluster.local flink-jobmanager.default.svc.cluster.local.svc.cluster.local flink-jobmanager.default.svc.cluster.local.cluster.local flink-jobmanager.default.svc.cluster.local.openstacklocal ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:4","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"③ options resolv.conf 文件的第一行是 options 其它项，常见配置是 ndots。ndots: 5 表示如果域名包含的 “.” 少于 5 个，则先添加 search 后缀，再使用绝对域名；如果域名包含的 “.” 大于等于 5 个，则先使用绝对域名，再添加 search 后缀。 #样例1 域名a.b.c.d.e，则域名的顺序如下 a.b.c.d.e.default.svc.cluster.local a.b.c.d.e.svc.cluster.local a.b.c.d.e.cluster.local a.b.c.d.e.openstacklocal a.b.c.d.e #样例2 域名a.b.c.d.e.f，则域名的顺序如下 a.b.c.d.e.f a.b.c.d.e.f.default.svc.cluster.local a.b.c.d.e.f.svc.cluster.local a.b.c.d.e.f.cluster.local a.b.c.d.e.f.openstacklocal 结合 flink-jobmanager.default.svc.cluster.local 域名解析慢的问题，由于该域名包含 “.” 等于 4，即少于 5，所以会首先依次添加后缀 default.svc.cluster.local、svc.cluster.local、cluster.local openstacklocal。因此，解决方案有 ①flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager；②ndots: 5 修改为 ndots: 4。 4 解决方案 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:5","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.1 使用简洁域名 将访问的域名从 flink-jobmanager.default.svc.cluster.local 修改为 flink-jobmanager，效果如下： curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.004 time_connect: 0.005 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:6","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"4.2 resolv.conf 配置更改 将 /etc/resolv.conf 的 options 配置 ndots: 5 修改为 ndots: 4，或者修改 deployment 的 yaml 配置，效果如下： # 修改 deployment 的 yaml 配置 # spec.spec.dnsConfig.options[].ndots dnsConfig: options: - name: ndots value: 4 - name: single-request-reopen curl -w \"@curl-format.txt\" -o /dev/null -l \"http://flink-jobmanager.default.svc.cluster.local:8091/jobs/overview\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 660 100 660 0 0 82038 0 --:--:-- --:--:-- --:--:-- 94285 time_melookup: 0.005 time_connect: 0.006 time_appconnect: 0.000 time_redirect: 0.000 time_pretransfer: 0.005 time_starttransfer: 0.008 ---------- time_total: 0.008 ","date":"2023-12-27","objectID":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/:2:7","tags":[""],"title":"coredns解析超时问题","uri":"/%E8%A7%A3%E6%9E%90%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"Nginx 502问题排查 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:0:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"nginx快速定位异常 错误信息 错误说明 “upstream prematurely（过早的） closed connection” 请求uri的时候出现的异常，是由于upstream还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略 “recv() failed (104: Connection reset by peer)” （1）服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉； （2）客户关掉了浏览器，而服务器还在给客户端发送数据； （3）浏览器端按了Stop “(111: Connection refused) while connecting to upstream” 用户在连接时，若遇到后端upstream挂掉或者不通，会收到该错误 “(111: Connection refused) while reading response header from upstream” 用户在连接成功后读取数据时，若遇到后端upstrream挂掉或者不通，会收到该错误 “(111: Connection refused) while sending request to upstream” Nginx和upstream连接成功后发送数据时，若遇到后端upstream挂掉或者不通，会收到该错误 “(110: Connection timed out) while connecting to upstream” nginx连接后面的upstream时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(110: Connection timed out) while reading response header from upstream” nginx读取来自upstream的响应头时超时 “(110: Connection timed out) while reading upstream” nginx读取来自upstream的响应时超时 “(104: Connection reset by peer) while connecting to upstream” upstream发送了RST，将连接重置 “upstream sent invalid header while reading response header from upstream” upstream发送的响应头无效 “upstream sent no valid HTTP/1.0 header while reading response header from upstream” upstream发送的响应头无效 “client intended to send too large body” 用于设置允许接受的客户端请求内容的最大值，默认值是1M，client发送的body超过了设置值 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:1:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Nginx日志状态码为502 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"负载均衡实例连接数超限(Connection reset by peer) 报错: recv() failed (104: Connection reset by peer) while reading response header from upstream 线上Nginx运行了n年后，业务发现有502错误的错误，通过排查发现这个502已经持续了很久很久，通过error日志发现报错是recv() failed (104: Connection reset by peer) while reading response header from upstream 业务架构 负载均衡: 均使用阿里云的CLB实例 日志排查 排查日志发现, 发现Istio Gateway没有收到Nginx的请求, 所以判断问题出在 Nginx到Istio Gateway 抓包 通过抓包发现，TCP连接建立之后，Nginx发送的HTTP请求并没有发送到Istio 502问题排查-wireshark-rest 解决 通过查看阿里云的SLB监控，发现内网Istio负载均衡有丢包的情况，于是对CLB实例进行了升配，修复了这个问题 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:1","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"Connection timed out upstream timed out (110: Connection timed out) while connecting to upstream connect() failed (110: Connection timed out) while connecting to upstream 对CLB实例升配后，虽然502少了特别多，但是还是有502的请求，报错是upstream timed out (110: Connection timed out) while connecting to upstream ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:2","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"解决方案 增加keepalive, 具体可以参考文档: https://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive 示例 upstream http_backend { server 127.0.0.1:8080; keepalive 16; } server { ... location /http/ { proxy_pass http://http_backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; ... } } 后续 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:2:3","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"为何SLB并发支持的这么小 https://www.alibabacloud.com/help/zh/slb/classic-load-balancer/user-guide/faq-about-clb#section-3kw-gvt-8go 场景描述：在使用少量长连接的业务场景下，转发分组中的系统服务器可能不会全部被分配到长连接，可能会出现CLB实例达不到QPS峰值的现象。 原理（原因）： 因为负载均衡系统通过集群部署的方式为负载均衡实例提供服务，所有外部的访问请求都将平均分散到这些负载均衡系统服务器上进行转发。所以，CLB实例的QPS峰值将被平均设定在多台系统服务器上。 单个系统服务器的QPS上限计算方法为：单个系统服务器QPS峰值=实例总QPS/（N-1）。N为转发分组中系统服务器的个数。例如您在控制台上购买了简约型I（slb.s1.small）规格的CLB实例，对应的QPS为1000，当多客户端同时使用时，总QPS可以达到1000 QPS。若系统服务器个数为8，那么单个系统服务器的最大QPS为1000/(8-1)=142 QPS。 推荐方案： 使用单客户端短连接进行压测。 根据实际业务情况减少连接复用。 升配CLB实例规格。具体操作，请参见按量付费（按规格计费）升降配。 使用ALB实例，此种方案下负载均衡实例具有足够的弹性。 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:3:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation"],"content":"参考文档 Nginx: Connection reset by peer 错误定位 nginx(二十九)error.log记录报错信息分析 ","date":"2023-12-19","objectID":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:4:0","tags":["nginx"],"title":"Nginx 502问题排查","uri":"/502%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":["documentation","windows"],"content":"powershell设置自动补全 notepad.exe $PROFILE hugo.exe completion powershell | Out-String | Invoke-Expression ","date":"2023-11-30","objectID":"/powershell-completion/:0:0","tags":["windows"],"title":"Powershell Completion","uri":"/powershell-completion/"},{"categories":["documentation","hugo"],"content":"hugo page first page ","date":"2023-11-28","objectID":"/first/:0:0","tags":["hugo"],"title":"First","uri":"/first/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"2019-08-02","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 73 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"2019-08-02","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"2019-08-02","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]